{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import gamma, norm\n",
    "from darts.models import RNNModel,RegressionModel , RandomForest, XGBModel\n",
    "import pywt\n",
    "from darts import TimeSeries\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "Step 1: Load and Prepare Data\n",
    "#Read and preprocess the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_gamma(precip: pd.Series) -> tuple:\n",
    "    non_zero = precip[precip > 0]\n",
    "    if len(non_zero) == 0:\n",
    "        return np.nan, np.nan, 1.0\n",
    "    shape, _, scale = gamma.fit(non_zero, floc=0)\n",
    "    zero_prob = (precip == 0).mean()\n",
    "    return shape, scale, zero_prob\n",
    "\n",
    "\n",
    "def compute_spi(roll: pd.Series, shape: float, scale: float, zero_prob: float) -> np.ndarray:\n",
    "    # probs = zero_prob + (1 - zero_prob) * gamma.cdf(roll, shape, scale=scale)\n",
    "    probs = np.where(\n",
    "    roll.isna(),\n",
    "    np.nan,\n",
    "    zero_prob + (1 - zero_prob) * gamma.cdf(roll, shape, scale=scale)\n",
    "    )\n",
    "    probs = np.clip(probs, 1e-10, 1 - 1e-10)\n",
    "    return norm.ppf(probs)\n",
    "\n",
    "\n",
    "def process_station(df_station: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_station = df_station.sort_values('ds').reset_index(drop=True)\n",
    "    # shape, scale, zero_prob = fit_gamma(df_station['precip'])\n",
    "    result = {'ds': df_station['ds']}\n",
    "\n",
    "    for s in [1, 3, 6, 9, 12, 24]:\n",
    "        roll = df_station['precip'].rolling(s, min_periods=s).sum()\n",
    "        shape, scale, zero_prob = fit_gamma(roll.dropna())\n",
    "\n",
    "        spi = compute_spi(roll, shape, scale, zero_prob)\n",
    "        # z-score manually\n",
    "        mean, std = np.nanmean(spi), np.nanstd(spi)\n",
    "        result[f'SPI_{s}'] = (spi - mean) / std\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df['station_id'] = df_station['station_id'].iloc[0]\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Load & aggregate to monthly totals\n",
    "df = (\n",
    "    pd.read_csv('../result/merged_data.csv', parse_dates=['data'])\n",
    "      .assign(ds=lambda d: d['data'].dt.to_period('M').dt.to_timestamp())\n",
    "      .groupby(['station_id', 'ds'])['rrr24']\n",
    "      .sum()\n",
    "      .reset_index(name='precip')\n",
    ")\n",
    "\n",
    "# Compute SPI for each station\n",
    "spi_list = [process_station(g) for _, g in df.groupby('station_id')]\n",
    "all_spi = pd.concat(spi_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d6455",
   "metadata": {},
   "source": [
    "class TaylorDiagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaylorDiagram:\n",
    "    def __init__(self, ref_std, fig=None, rect=111, label='Reference'):\n",
    "        self.ref_std = ref_std\n",
    "\n",
    "        self.fig = fig if fig is not None else plt.figure()\n",
    "        self.ax = self.fig.add_subplot(rect, polar=True)\n",
    "        self.ax.set_theta_direction(-1)\n",
    "        self.ax.set_theta_offset(np.pi / 2)\n",
    "\n",
    "        self.sample_points = []\n",
    "\n",
    "        # Set up axis\n",
    "        self.ax.set_thetamin(0)\n",
    "        self.ax.set_thetamax(90)\n",
    "        self.ax.set_ylim(0, 1.5 * ref_std)\n",
    "\n",
    "        # Set up correlation grid\n",
    "        self._setup_axes()\n",
    "\n",
    "        # Reference point\n",
    "        self.ax.plot([0], [ref_std], 'k*', markersize=10, label=label)\n",
    "\n",
    "    def _setup_axes(self):\n",
    "        corrs = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99, 1.0])\n",
    "        angles = np.arccos(corrs)\n",
    "        self.ax.set_thetagrids(np.degrees(angles), labels=[f\"{c:.2f}\" for c in corrs])\n",
    "        self.ax.set_ylabel('Standard Deviation')\n",
    "\n",
    "    def add_sample(self, stddev, corrcoef, label, marker='o', c='C0'):\n",
    "        theta = np.arccos(corrcoef)\n",
    "        point = self.ax.plot(theta, stddev, marker, label=label, color=c)\n",
    "        self.sample_points.append(point)\n",
    "\n",
    "    def add_contours(self, levels=5, **kwargs):\n",
    "        rs, ts = np.meshgrid(\n",
    "            np.linspace(0, self.ax.get_ylim()[1]),\n",
    "            np.linspace(0, np.pi / 2)\n",
    "        )\n",
    "        rms = np.sqrt(\n",
    "            self.ref_std**2 + rs**2 - 2 * self.ref_std * rs * np.cos(ts)\n",
    "        )\n",
    "        contours = self.ax.contour(ts, rs, rms, levels=levels, **kwargs)\n",
    "        return contours\n",
    "    \n",
    "    def plot_spi_trend(historical: TimeSeries, forecast: TimeSeries, title='SPI Trend'):\n",
    "        full_series = historical.append(forecast)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        full_series.plot(label=\"SPI\")\n",
    "        plt.axvline(x=historical.end_time(), color='r', linestyle='--', label=\"Forecast Start\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"SPI\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926e076",
   "metadata": {},
   "source": [
    "Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WBBLSTMModel:\n",
    "    def __init__(self, wavelet='db1', level=1, **lstm_kwargs):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "        self.lstm_model = RNNModel(model='LSTM', **lstm_kwargs)\n",
    "        \n",
    "    def wavelet_decompose(self, series: TimeSeries):\n",
    "        coeffs = pywt.wavedec(series.values().flatten(), self.wavelet, level=self.level)\n",
    "        return coeffs\n",
    "    \n",
    "    def wavelet_reconstruct(self, coeffs):\n",
    "        return pywt.waverec(coeffs, self.wavelet)\n",
    "    \n",
    "    def fit(self, series: TimeSeries):\n",
    "        # Decompose\n",
    "        coeffs = self.wavelet_decompose(series)\n",
    "        approx = coeffs[0]  # Use approximation coeffs as input to LSTM\n",
    "        approx_series = TimeSeries.from_values(approx.reshape(-1, 1))\n",
    "        \n",
    "        # Fit the internal LSTM model\n",
    "        self.lstm_model.fit(approx_series)\n",
    "\n",
    "    def predict(self, n,series):\n",
    "        pred = self.lstm_model.predict(n)\n",
    "        return pred\n",
    "    def save(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Save LSTM model\n",
    "        self.lstm_model.save(os.path.join(path, \"lstm_model\"))\n",
    "\n",
    "        # Save wavelet settings and kwargs\n",
    "        meta = {\n",
    "            'wavelet': self.wavelet,\n",
    "            'level': self.level,\n",
    "            'lstm_kwargs': self.lstm_kwargs\n",
    "        }\n",
    "        with open(os.path.join(path, \"meta.json\"), \"w\") as f:\n",
    "            json.dump(meta, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(os.path.join(path, \"meta.json\"), \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        instance = cls(**meta)\n",
    "        instance.lstm_model = RNNModel.load(os.path.join(path, \"lstm_model\"))\n",
    "        return instance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "window_size = 12\n",
    "num_epochs=300\n",
    "model_constructors = {\n",
    "    'LSTM'      : lambda: RNNModel(model='LSTM', input_chunk_length=window_size, output_chunk_length=6,\n",
    "                                   hidden_dim=25, n_rnn_layers=2, dropout=0.1,\n",
    "                                   batch_size=16, n_epochs=num_epochs, optimizer_kwargs={'lr':1e-3},\n",
    "                                   random_state=SEED),\n",
    "    'SVR'       : lambda: RegressionModel(model=SVR(),lags=window_size),\n",
    "    'RandomRF'  : lambda: RandomForest(lags=window_size, n_estimators=100, random_state=SEED),\n",
    "    'ExtraTF'   : lambda: XGBModel(lags=window_size, objective='reg:squarederror', random_state=SEED),\n",
    "    'WBBLSTM'   : lambda: WBBLSTMModel(\n",
    "        wavelet='db1',\n",
    "        level=1,\n",
    "        input_chunk_length=12,\n",
    "        output_chunk_length=6,\n",
    "        hidden_dim=25,\n",
    "        n_rnn_layers=2,\n",
    "        dropout=0.1,\n",
    "        batch_size=16,\n",
    "        n_epochs=num_epochs,\n",
    "        optimizer_kwargs={'lr':1e-3},\n",
    "        random_state=SEED\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def compute_metrics(obs: np.ndarray, pred: np.ndarray):\n",
    "    rmse = np.sqrt(mean_squared_error(obs, pred))\n",
    "    std_obs, std_pred = np.std(obs), np.std(pred)\n",
    "    corr = np.corrcoef(obs, pred)[0,1]\n",
    "    return std_obs, std_pred, corr, rmse\n",
    "\n",
    "results = {}  \n",
    "base_dir = \"all_results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "for station_id, group in all_spi.groupby('station_id'):\n",
    "    results[station_id] = {}\n",
    "    for col in [c for c in group.columns if c.startswith(\"SPI_\")]:\n",
    "\n",
    "        print(f\"\\n=== Station {station_id} | {col} ===\")\n",
    "        sub_df = group[['ds', col]].dropna()\n",
    "\n",
    "        series = TimeSeries.from_dataframe(sub_df, time_col='ds', value_cols=col)\n",
    "        \n",
    "        # split once per series\n",
    "        train, val = series.split_before(0.8)\n",
    "\n",
    "\n",
    "        model_folder = os.path.join(base_dir, \"{station_id}/{col}\")\n",
    "\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "        model_stats = {}\n",
    "        forecasts = {}\n",
    "        for name, mk in model_constructors.items():\n",
    "            model_file = os.path.join(model_folder, f\"{name}.pkl\")\n",
    "\n",
    "            if os.path.exists(model_file):\n",
    "                print(f\"Model {name} already trained. Skipping.\")\n",
    "                # model = load(model_file)\n",
    "                model_class = {\n",
    "                    'LSTM': RNNModel,\n",
    "                    'SVR': RegressionModel,\n",
    "                    'RandomRF': RandomForest,\n",
    "                    'ExtraTF': XGBModel,\n",
    "                    'WBBLSTM': WBBLSTMModel\n",
    "                }[name]\n",
    "                model = model_class.load(model_file)\n",
    "\n",
    "            else:\n",
    "                print(f\" Training {name}…\", end='')\n",
    "                model = mk()\n",
    "                model.fit(train)\n",
    "                model.save(model_file)\n",
    "\n",
    "                print(\" saved.\")\n",
    "\n",
    "\n",
    "            forecast = model.predict(len(val), series=train)\n",
    "            o = val.values().flatten()\n",
    "            p = forecast.values().flatten()\n",
    "            std_o, std_p, corr, rmse = compute_metrics(o, p)\n",
    "            model_stats[name] = (std_o, std_p, corr, rmse)\n",
    "        \n",
    "        results[station_id][col] = model_stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a3e1",
   "metadata": {},
   "source": [
    "Loop over stations & timescales, train & forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae148b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import os\n",
    "\n",
    "\n",
    "base_dir = \"all_results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "for station_id, group in all_spi.groupby('station_id'):\n",
    "        station_dir = os.path.join(base_dir, str(station_id))\n",
    "        os.makedirs(station_dir, exist_ok=True)\n",
    "        # for col in [c for c in group.columns if c.startswith(\"SPI_\")]:\n",
    "\n",
    "            # Taylor Diagram\n",
    "            ref_std = list(model_stats.values())[0][0]\n",
    "            fig = plt.figure(figsize=(6, 6))\n",
    "            taylor = TaylorDiagram(ref_std, fig, label='Observed')\n",
    "            for name, (std_o, std_p, corr, rmse) in model_stats.items():\n",
    "                taylor.add_sample(std_p, corr, label=name)\n",
    "            taylor.add_contours(levels=5, colors='0.5')\n",
    "            plt.legend()\n",
    "            plt.title(f\"Taylor Diagram: Station {station_id} | {col}\")\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(f\"{station_dir}/taylor_{col}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # plt.show()\n",
    "\n",
    "            # Best model\n",
    "            best_model = min(model_stats.items(), key=lambda x: x[1][3])[0]\n",
    "            print(f\"Best model for {station_id} | {col}: {best_model}\")\n",
    "            # best = model_constructors[best_model]()\n",
    "            # best.fit(series)\n",
    "            model_file = os.path.join(model_folder, f\"{best_model}.pkl\")\n",
    "\n",
    "            if os.path.exists(model_file):\n",
    "                print(f\"Model {name} already trained. Skipping.\")\n",
    "                # model = load(model_file)\n",
    "                model_class = {\n",
    "                    'LSTM': RNNModel,\n",
    "                    'SVR': RegressionModel,\n",
    "                    'RandomRF': RandomForest,\n",
    "                    'ExtraTF': XGBModel,\n",
    "                    'WBBLSTM': WBBLSTMModel\n",
    "                }[name]\n",
    "                best = model_class.load(model_file)\n",
    "\n",
    "            # Forecast to 2050\n",
    "            horizon = (pd.Timestamp(\"2050-12-01\") - series.end_time()).days // 30\n",
    "            future = best.predict(horizon)\n",
    "            plot_spi_trend(series, future, title=f\"SPI Forecast to 2050 — {station_id} | {col}\")\n",
    "\n",
    "            # Plot val vs forecast\n",
    "            pred = forecasts[best_model]\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            val.plot(label=\"Observed\")\n",
    "            pred.plot(label=f\"Forecast - {best_model}\")\n",
    "            plt.title(f\"Validation vs Prediction — {station_id} | {col}\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Prepare SPI dataframe for heatmap + category\n",
    "            spi_series = sub_df.copy()\n",
    "            spi_series['year'] = pd.to_datetime(spi_series['ds']).dt.year\n",
    "            spi_series['month'] = pd.to_datetime(spi_series['ds']).dt.month\n",
    "\n",
    "            # SPI heatmap\n",
    "            heatmap_data = spi_series.pivot_table(index='year', columns='month', values=col)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(heatmap_data, cmap='rocket', center=0, annot=True, fmt=\".2f\")\n",
    "            plt.title(f\"SPI Heatmap — {col} — {station_id}\")\n",
    "            plt.xlabel(\"Month\")\n",
    "            plt.ylabel(\"Year\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Categorical SPI scatter\n",
    "            spi_series['category'] = pd.cut(spi_series[col], bins=[-np.inf, -1, 1, np.inf], labels=['Dry', 'Normal', 'Wet'])\n",
    "            colors = {'Dry': 'red', 'Normal': 'gray', 'Wet': 'blue'}\n",
    "\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            plt.scatter(spi_series['ds'], spi_series[col], c=spi_series['category'].map(colors), label='SPI Category')\n",
    "            plt.axhline(0, color='black', lw=1, linestyle='--')\n",
    "            plt.title(f\"SPI Categories: Dry / Normal / Wet — {station_id} | {col}\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"SPI Value\")\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"✔️ Done with {station_id} | {col}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278cbb0",
   "metadata": {},
   "source": [
    "Plotting a Taylor Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def taylor_diagram(std_ref, models_stats, title=\"Taylor Diagram\"):\n",
    "    \"\"\"\n",
    "    std_ref : float\n",
    "        standard deviation of reference.\n",
    "    models_stats : dict\n",
    "        { model_name: (std_ref, std_model, corrcoef, rmse) }\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    ax.set_theta_zero_location('E')\n",
    "    ax.set_theta_direction(-1)\n",
    "    \n",
    "    # radial limits\n",
    "    maxstd = max(v[1] for v in models_stats.values()) * 1.1\n",
    "    ax.set_rmax(maxstd)\n",
    "    ax.set_rticks(np.linspace(0, maxstd, 4))\n",
    "    ax.set_rlabel_position(135)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # plot reference point\n",
    "    ax.plot(0, std_ref, 'k*', label='Reference')\n",
    "    \n",
    "    # plot each model\n",
    "    for name, (std_o, std_p, corr, rmse) in models_stats.items():\n",
    "        theta = np.arccos(corr)  # angle from zero\n",
    "        ax.plot(theta, std_p, 'o', label=f\"{name} (RMSE={rmse:.2f})\")\n",
    "    \n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for station_id, timescale_dict in results.items():\n",
    "    for timescale, stats in timescale_dict.items():\n",
    "        std_ref = next(iter(stats.values()))[0]  # same for every model\n",
    "        title = f\"Station {station_id} | {timescale}\"\n",
    "        taylor_diagram(std_ref, stats, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import RNNModel\n",
    "spi_series = all_spi_data[40708].dropna()\n",
    "\n",
    "ts = TimeSeries.from_dataframe(spi_series, 'ds', 'SPI_3')\n",
    "\n",
    "model = RNNModel(\n",
    "    model='LSTM',\n",
    "    input_chunk_length=12,    # e.g. use one year of past data\n",
    "    output_chunk_length=6,    # forecast 6 months at a time\n",
    "    hidden_dim=25,           # size of hidden layer (try adjusting)\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    batch_size=16,\n",
    "    n_epochs=300,             # increase if needed for better convergence\n",
    "    optimizer_kwargs={'lr': 1e-3},\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb166cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ts, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# End date of your time series:\n",
    "last_date = ts.end_time().to_pydatetime()\n",
    "\n",
    "# Calculate forecast horizon (number of months until January 2050).\n",
    "# (Assuming you want to forecast up to and including December 2049,\n",
    "#  so that the new data start from January 2050.)\n",
    "forecast_steps = (2050 - last_date.year) * 12 - (last_date.month - 1)\n",
    "\n",
    "print(f\"Forecasting {forecast_steps} months into the future.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict the future SPI until 2050.\n",
    "forecast = model.predict(n=forecast_steps)\n",
    "\n",
    "# Plot the original time series and the forecast.\n",
    "plt.figure(figsize=(14, 7))\n",
    "ts.plot(label='Observed SPI', lw=2)\n",
    "forecast.plot(label='Forecast SPI', lw=2)\n",
    "\n",
    "plt.title(\"SPI Forecast with LSTM (Darts) until 2050\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"SPI Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd684b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming ts is a pandas Series with datetime index\n",
    "# df = ts.to_frame(name='SPI')\n",
    "spi_series = all_spi_data[40708].dropna()\n",
    "\n",
    "\n",
    "spi_series['year'] = spi_series['ds'].dt.year\n",
    "spi_series['month'] = spi_series['ds'].dt.month\n",
    "\n",
    "# Pivot to heatmap form: each row = year, columns = month\n",
    "heatmap_data = spi_series.pivot_table(index='year', columns='month', values='SPI_3')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, cmap='rocket', center=0, annot=True, fmt=\".2f\")\n",
    "plt.title(\"SPI Heatmap: Dry (-) and Wet (+) Conditions by Month and Year\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_series['category'] = pd.cut(spi_series['SPI_3'], bins=[-np.inf, -1, 1, np.inf], labels=['Dry', 'Normal', 'Wet'])\n",
    "\n",
    "colors = {'Dry': 'red', 'Normal': 'gray', 'Wet': 'blue'}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(spi_series.index, spi_series['SPI_3'], c=spi_series['category'].map(colors), label='SPI by Category')\n",
    "plt.axhline(0, color='black', lw=1, linestyle='--')\n",
    "plt.title(\"SPI Categorized as Dry, Normal, and Wet Days\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"SPI Value\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
