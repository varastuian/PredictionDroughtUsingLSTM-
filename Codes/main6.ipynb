{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from scipy.stats import gamma, norm\n",
    "from darts.models import RNNModel,RegressionModel , RandomForest, XGBModel\n",
    "from darts.metrics import rmse, mape,mae, smape\n",
    "# from darts.utils.preprocessing import Scaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "# from darts.ad import ThresholdAD\n",
    "from darts import TimeSeries\n",
    "\n",
    "import pywt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "\n",
    "Read and preprocess the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b43e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"ds\"])\n",
    "    df.rename(columns={\n",
    "        \"ds\": \"date\",\n",
    "        \"precip\": \"rainfall\",\n",
    "        \"SPI_1\": \"spi1\",\n",
    "        \"SPI_3\": \"spi3\",\n",
    "        \"SPI_6\": \"spi6\",\n",
    "        \"SPI_9\": \"spi9\",\n",
    "        \"SPI_12\": \"spi12\",\n",
    "        \"SPI_24\": \"spi24\",\n",
    "        \"station_id\": \"station\"\n",
    "    }, inplace=True)\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_seasonality(series):\n",
    "    decomposition = seasonal_decompose(series, model='additive', period=12, extrapolate_trend='freq')\n",
    "    deseasonalized = series - decomposition.seasonal\n",
    "    return deseasonalized, decomposition.seasonal\n",
    "\n",
    "\n",
    "def make_features(df, timescale, lags=12):\n",
    "    # Use SPI timescale and rainfall as covariates\n",
    "    data = df[[f\"spi{timescale}\", \"rainfall\"]].copy()\n",
    "    data = data.rename(columns={f\"spi{timescale}\": \"spi\"})\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # remove seasonality\n",
    "    # data['spi_deseason'] = remove_seasonality(data['spi'])\n",
    "    data['spi_deseason'], seasonal = remove_seasonality(data['spi'])\n",
    "    data['seasonal'] = seasonal\n",
    "\n",
    "    # lag features\n",
    "    for lag in range(1, lags+1):\n",
    "        data[f'spi_lag_{lag}'] = data['spi_deseason'].shift(lag)\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d6455",
   "metadata": {},
   "source": [
    "class TaylorDiagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaylorDiagram:\n",
    "    def __init__(self, ref_std, fig=None, rect=111, label='Reference'):\n",
    "        self.ref_std = ref_std\n",
    "        self.sample_points = []\n",
    "\n",
    "        self.fig = fig if fig is not None else plt.figure(figsize=(8, 6))\n",
    "        self.ax = self.fig.add_subplot(rect, polar=True)\n",
    "\n",
    "        # Configure polar axes\n",
    "        self.ax.set_theta_zero_location('E')\n",
    "\n",
    "        self.ax.set_theta_direction(-1)\n",
    "        self.ax.set_theta_offset(np.pi / 2)\n",
    "        self.ax.set_ylim(0, 1.5 * ref_std)\n",
    "        self.ax.set_thetamin(0)\n",
    "        self.ax.set_thetamax(90)\n",
    "\n",
    "        # Set up correlation coefficient grid\n",
    "        self._setup_axes()\n",
    "\n",
    "        # Plot reference point\n",
    "        self.ax.plot([0], [ref_std], 'k*', markersize=12, label=label)\n",
    "\n",
    "    def _setup_axes(self):\n",
    "        corrs = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99, 1.0])\n",
    "        angles = np.arccos(corrs)\n",
    "\n",
    "        self.ax.set_thetagrids(np.degrees(angles), labels=[f\"{c:.2f}\" for c in corrs], fontsize=10)\n",
    "        self.ax.set_rlabel_position(135)\n",
    "        self.ax.set_ylabel('Standard Deviation', fontsize=12)\n",
    "\n",
    "        # Add radial grid lines manually\n",
    "        for angle in angles:\n",
    "            self.ax.plot([angle, angle], [0, self.ax.get_ylim()[1]], color='lightgray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    def add_sample(self, stddev, corrcoef, label, marker='o', color=None):\n",
    "        theta = np.arccos(corrcoef)\n",
    "        point, = self.ax.plot(theta, stddev, marker=marker, label=label, color=color, markersize=8)\n",
    "        self.sample_points.append(point)\n",
    "\n",
    "    def add_contours(self, levels=6, cmap='coolwarm', linewidths=1.2):\n",
    "        rs, ts = np.meshgrid(\n",
    "            np.linspace(0, self.ax.get_ylim()[1], 300),\n",
    "            np.linspace(0, np.pi / 2, 300)\n",
    "        )\n",
    "        rms = np.sqrt(\n",
    "            self.ref_std**2 + rs**2 - 2 * self.ref_std * rs * np.cos(ts)\n",
    "        )\n",
    "        contours = self.ax.contour(\n",
    "            ts, rs, rms,\n",
    "            levels=np.linspace(0, self.ax.get_ylim()[1], levels),\n",
    "            cmap=cmap,\n",
    "            linewidths=linewidths\n",
    "        )\n",
    "        self.fig.colorbar(contours, ax=self.ax, pad=0.1, orientation='vertical', label=\"RMS Difference\")\n",
    "        return contours\n",
    "\n",
    "    def show(self, title='Taylor Diagram'):\n",
    "        self.ax.set_title(title, fontsize=14, pad=20)\n",
    "        self.ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926e076",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Define Models and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bb81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WBBLSTMModel:\n",
    "\n",
    "    def __init__(self, wavelet='db1', level=2, **lstm_kwargs):\n",
    "        self.wavelet     = wavelet\n",
    "        self.level       = level\n",
    "        self.lstm_kwargs = lstm_kwargs\n",
    "\n",
    "    def _denoise(self, series: TimeSeries) -> TimeSeries:\n",
    "        arr    = series.values().flatten()\n",
    "        coeffs = pywt.wavedec(arr, self.wavelet, level=self.level)\n",
    "        # zero detail coeffs\n",
    "        for i in range(1, len(coeffs)):\n",
    "            coeffs[i] = np.zeros_like(coeffs[i])\n",
    "        denoised = pywt.waverec(coeffs, self.wavelet)\n",
    "        # trim padding if any\n",
    "        denoised = denoised[:len(series)]\n",
    "        return TimeSeries.from_times_and_values(\n",
    "            series.time_index,\n",
    "            denoised.reshape(-1,1).astype(np.float32)\n",
    "        )\n",
    "\n",
    "    def fit(self, series: TimeSeries, future_covariates: TimeSeries = None):\n",
    "        denoised = self._denoise(series)\n",
    "        self.lstm = RNNModel(model='LSTM', **self.lstm_kwargs)\n",
    "        # drop covariates completely\n",
    "        self.lstm.fit(denoised,future_covariates=future_covariates)\n",
    "\n",
    "    def predict(self, n, series: TimeSeries, future_covariates: TimeSeries = None):\n",
    "        denoised = self._denoise(series)\n",
    "        # predict only on the denoised series\n",
    "        return self.lstm.predict(n, series=denoised,future_covariates=future_covariates)\n",
    "\n",
    "    def save(self, path):\n",
    "            self.lstm.save(path)\n",
    "            meta = {'wavelet':self.wavelet, 'level':self.level, 'lstm_kwargs':self.lstm_kwargs}\n",
    "            with open(os.path.join(os.path.dirname(path),\"WBCovLSTMmeta.json\"),\"w\") as f:\n",
    "                json.dump(meta, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        meta = json.load(open(os.path.join(os.path.dirname(path),\"WBCovLSTMmeta.json\")))\n",
    "        inst = cls(wavelet=meta['wavelet'], level=meta['level'], **meta['lstm_kwargs'])\n",
    "        inst.lstm = RNNModel.load(path)\n",
    "        return inst\n",
    "\n",
    "\n",
    "window_size = 24\n",
    "num_epochs=300\n",
    "horizon = 1 \n",
    "\n",
    "\n",
    "model_constructors = {\n",
    "       'ExtraTF': lambda: XGBModel(\n",
    "         lags=window_size,\n",
    "        lags_future_covariates=[0],\n",
    "         output_chunk_length=horizon,\n",
    "         random_state=SEED,\n",
    "         objective='reg:squarederror'\n",
    "    ),\n",
    "    'RandomRF': lambda: RandomForest(\n",
    "         lags=window_size,\n",
    "         lags_future_covariates=[0],\n",
    "         output_chunk_length=horizon,\n",
    "         n_estimators=100,\n",
    "         criterion=\"absolute_error\",\n",
    "         random_state=SEED\n",
    "    ),\n",
    "    'SVR': lambda: RegressionModel(\n",
    "         model=SVR(kernel='rbf'),\n",
    "         lags_future_covariates=[0],\n",
    "         lags=window_size,\n",
    "         output_chunk_length=horizon\n",
    "    ),\n",
    "    'LSTM': lambda: RNNModel(\n",
    "         model='LSTM',\n",
    "         input_chunk_length=window_size,\n",
    "         output_chunk_length=horizon,\n",
    "         hidden_dim=64,\n",
    "         n_rnn_layers=2,\n",
    "         dropout=0.1,\n",
    "         batch_size=16,\n",
    "         n_epochs=num_epochs,\n",
    "         optimizer_kwargs={'lr':1e-3},\n",
    "         random_state=SEED\n",
    "    ),\n",
    "    'WBBLSTM'   : lambda: WBBLSTMModel(\n",
    "        wavelet='db1',\n",
    "        level=2,\n",
    "        input_chunk_length=window_size,\n",
    "        output_chunk_length=horizon,\n",
    "        hidden_dim=64,\n",
    "        n_rnn_layers=2,\n",
    "        dropout=0.1,\n",
    "        batch_size=16,\n",
    "        n_epochs=num_epochs,\n",
    "        optimizer_kwargs={'lr':1e-3},\n",
    "        random_state=SEED\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4fc8a",
   "metadata": {},
   "source": [
    "main loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16601ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Station 40700 | 1 ===\n",
      "!!!@@@------Training ExtraTFâ€¦ saved.\n",
      "!!!@@@------Training RandomRFâ€¦ saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 53.8 K | train\n",
      "6 | V               | Linear           | 65     | train\n",
      "-------------------------------------------------------------\n",
      "53.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.8 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!@@@------Training SVRâ€¦ saved.\n",
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 112.83it/s, train_loss=0.00285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 112.34it/s, train_loss=0.00285]\n",
      " saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.38it/s]\n",
      "!!!@@@------Training WBBLSTMâ€¦"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 53.8 K | train\n",
      "6 | V               | Linear           | 65     | train\n",
      "-------------------------------------------------------------\n",
      "53.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.8 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 116.58it/s, train_loss=0.00106] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 116.06it/s, train_loss=0.00106]\n",
      " saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.80it/s]\n",
      "std_p: 0.71, corr: 0.84, rmse: 0.45, mae_v: 0.34,sm: 86.09 - ExtraTF \n",
      "std_p: 0.67, corr: 0.82, rmse: 0.47, mae_v: 0.35,sm: 77.02 - RandomRF \n",
      "std_p: 0.66, corr: 0.84, rmse: 0.45, mae_v: 0.32,sm: 70.36 - SVR \n",
      "std_p: 0.74, corr: 0.89, rmse: 0.44, mae_v: 0.35,sm: 83.37 - LSTM \n",
      "std_p: 0.16, corr: 0.26, rmse: 0.81, mae_v: 0.62,sm: 152.35 - WBBLSTM \n",
      "âœ”ï¸ Done with 40700 | 1\n",
      "\n",
      "\n",
      "=== Station 40700 | 3 ===\n",
      "!!!@@@------Training ExtraTFâ€¦ saved.\n",
      "!!!@@@------Training RandomRFâ€¦ saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 53.8 K | train\n",
      "6 | V               | Linear           | 65     | train\n",
      "-------------------------------------------------------------\n",
      "53.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.8 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!@@@------Training SVRâ€¦ saved.\n",
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 121.51it/s, train_loss=0.00486]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 121.03it/s, train_loss=0.00486]\n",
      " saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.70it/s]\n",
      "!!!@@@------Training WBBLSTMâ€¦"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 53.8 K | train\n",
      "6 | V               | Linear           | 65     | train\n",
      "-------------------------------------------------------------\n",
      "53.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.8 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 111.22it/s, train_loss=0.00198]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 110.80it/s, train_loss=0.00198]\n",
      " saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.90it/s]\n",
      "std_p: 0.65, corr: 0.45, rmse: 0.85, mae_v: 0.62,sm: 127.30 - ExtraTF \n",
      "std_p: 0.47, corr: 0.70, rmse: 0.62, mae_v: 0.39,sm: 99.40 - RandomRF \n",
      "std_p: 0.36, corr: 0.68, rmse: 0.67, mae_v: 0.42,sm: 101.98 - SVR \n",
      "std_p: 0.49, corr: 0.71, rmse: 0.68, mae_v: 0.45,sm: 106.05 - LSTM \n",
      "std_p: 0.28, corr: 0.49, rmse: 0.79, mae_v: 0.54,sm: 131.50 - WBBLSTM \n",
      "âœ”ï¸ Done with 40700 | 3\n",
      "\n",
      "\n",
      "=== Station 40700 | 6 ===\n",
      "Model ExtraTF already trained. Skipping.\n",
      "!!!@@@-----Loading ExtraTF model from results/40700/6/ExtraTFâ€¦\n",
      "Model RandomRF already trained. Skipping.\n",
      "!!!@@@-----Loading RandomRF model from results/40700/6/RandomRFâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SVR already trained. Skipping.\n",
      "!!!@@@-----Loading SVR model from results/40700/6/SVRâ€¦\n",
      "Model LSTM already trained. Skipping.\n",
      "!!!@@@-----Loading LSTM model from results/40700/6/LSTMâ€¦\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.53it/s]\n",
      "Model WBBLSTM already trained. Skipping.\n",
      "!!!@@@-----Loading WBBLSTM model from results/40700/6/WBBLSTMâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.58it/s]\n",
      "std_p: 0.42, corr: 0.52, rmse: 0.70, mae_v: 0.56,sm: 117.47 - ExtraTF \n",
      "std_p: 0.55, corr: 0.86, rmse: 0.45, mae_v: 0.34,sm: 79.00 - RandomRF \n",
      "std_p: 0.42, corr: 0.88, rmse: 0.55, mae_v: 0.41,sm: 86.30 - SVR \n",
      "std_p: 0.35, corr: 0.78, rmse: 0.81, mae_v: 0.63,sm: 138.45 - LSTM \n",
      "std_p: 0.26, corr: 0.79, rmse: 0.76, mae_v: 0.60,sm: 137.64 - WBBLSTM \n",
      "âœ”ï¸ Done with 40700 | 6\n",
      "\n",
      "ðŸ“„ Results saved as CSV: results/results_summary.csv\n"
     ]
    }
   ],
   "source": [
    "results = {}  \n",
    "base_dir = \"results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "df = load_data('./finaldata.csv')\n",
    "stations = df['station'].unique()\n",
    "summary = []\n",
    "\n",
    "def model_score(stats):\n",
    "    std_o, std_p, corr, rmse, mape_val, _ = stats\n",
    "    std_diff = abs(std_o - std_p)\n",
    "    return (\n",
    "        rmse + mape_val + std_diff - corr  \n",
    "    )\n",
    "\n",
    "plot_start = pd.Timestamp(\"2017-01-01\")\n",
    "plot_end   = pd.Timestamp(\"2024-04-01\")\n",
    "\n",
    "# for st in stations:\n",
    "for st in [40700]:\n",
    "    df_st = df[df['station'] == st].copy()\n",
    "    for ts in [1,3,6]:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        results[st] = {}\n",
    "        data = make_features(df_st, ts)\n",
    "        print(f\"\\n=== Station {st} | {ts} ===\")\n",
    "        target_series = TimeSeries.from_series(data[\"spi_deseason\"])\n",
    "        target_series = target_series.astype(np.float32)  \n",
    "\n",
    "        covariates = TimeSeries.from_dataframe(\n",
    "            data[[\"rainfall\"] + [f\"spi_lag_{i}\" for i in range(1, 13)]]\n",
    "        )\n",
    "        covariates = covariates.astype(np.float32)\n",
    "        target_series, covariates = target_series.slice_intersect(covariates), covariates.slice_intersect(target_series)\n",
    "\n",
    "        train, val = target_series.split_before(0.8)\n",
    "        target_series.slice(plot_start, plot_end).plot(label=\"Observed\", lw=2)\n",
    "\n",
    "        model_folder = os.path.join(base_dir, f\"{st}/{ts}\")\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "        model_stats = {}\n",
    "        forecasts = {}\n",
    "        for name, mk in model_constructors.items():\n",
    "            model_path = os.path.join(model_folder, name)\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"Model {name} already trained. Skipping.\")\n",
    "                model_class = {\n",
    "                    'LSTM': RNNModel,\n",
    "                    'SVR': RegressionModel,\n",
    "                    'RandomRF': RandomForest,\n",
    "                    'ExtraTF': XGBModel,\n",
    "                    'WBBLSTM': WBBLSTMModel\n",
    "                }[name]\n",
    "                print(f\"!!!@@@-----Loading {name} model from {model_path}â€¦\")\n",
    "                model = model_class.load(model_path)\n",
    "\n",
    "            else:\n",
    "                print(f\"!!!@@@------Training {name}â€¦\", end='')\n",
    "                model = mk()\n",
    "                model.fit(series=train, future_covariates=covariates)\n",
    "                model.save(model_path)\n",
    "\n",
    "                print(\" saved.\")\n",
    "\n",
    "            forecast = model.predict(len(val), series=train, future_covariates=covariates)\n",
    "            forecast_zoom = forecast.slice(plot_start, plot_end)\n",
    "            forecast_zoom.plot(label=name, lw=1.8)\n",
    "            o = val.values().flatten()\n",
    "            p = forecast.values().flatten()\n",
    "\n",
    "            corr = pearsonr(o, p)[0]\n",
    "            if corr < 0:\n",
    "                print('negative corr')\n",
    "                p = -p\n",
    "                corr = -corr\n",
    "\n",
    "\n",
    "            rm   = np.sqrt(mean_squared_error(o, p))\n",
    "            mae_v= mean_absolute_error(o, p)\n",
    "            sm   = np.mean(2 * np.abs(o-p) / (np.abs(o)+np.abs(p))) * 100\n",
    "\n",
    "            model_stats[name] = (np.std(o), np.std(p), corr, rm, mae_v, sm)\n",
    "            \n",
    "            forecasts[name] = forecast\n",
    "\n",
    "\n",
    "        plt.title(f\"Model Comparison â€” Station {st} | Timescale: SPI-{ts}\", fontsize=14)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"SPI Value\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(model_folder, f\"all_models_{st}_{ts}.png\"), dpi=300)\n",
    "        plt.close()\n",
    "        results[st][f\"spi_{ts}\"] = model_stats\n",
    "        for model_name, stats in model_stats.items():\n",
    "                        std_o, std_p, corr, rmse, mae, smape = stats\n",
    "                        score = model_score(stats)\n",
    "                        summary.append({\n",
    "                            'Station': st,\n",
    "                            'Timescale': ts,\n",
    "                            'Model': model_name,\n",
    "                            'STD_Obs': std_o,\n",
    "                            'STD_Pred': std_p,\n",
    "                            'Corr': corr,\n",
    "                            'RMSE': rmse,\n",
    "                            'MAE': mae,\n",
    "                            'SMAPE': smape,\n",
    "                            'Score': score,\n",
    "                        })\n",
    "\n",
    "        # Taylor Diagram\n",
    "        ref_std = list(model_stats.values())[0][0]\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        taylor = TaylorDiagram(ref_std, fig, label='Observed')\n",
    "        for name, (std_o, std_p, corr, rmse,mae_v,sm) in model_stats.items():\n",
    "            print(f\"std_p: {std_p:.2f}, corr: {corr:.2f}, rmse: {rmse:.2f}, mae_v: {mae_v:.2f},sm: {sm:.2f} - {name} \")\n",
    "            if np.isnan(std_p) or np.isnan(corr) or std_p == 0:\n",
    "                print(f\"âš ï¸ Skipping {name} due to invalid metrics.\")\n",
    "                continue\n",
    "            taylor.add_sample(std_p, corr, label=name)\n",
    "        taylor.add_contours()\n",
    "        plt.legend()\n",
    "        plt.title(f\"Taylor Diagram: Station {st} | {ts}\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(model_folder, f\"taylor_{st}_{ts}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Best model\n",
    "        best_model = min(model_stats.items(), key=lambda x: model_score(x[1]))[0]\n",
    "#         best_model = min(\n",
    "#     model_stats.items(),\n",
    "#     key=lambda item: (item[1][3], -item[1][2])\n",
    "# )[0]\n",
    "\n",
    "        # Plot val vs pred\n",
    "        _, val = target_series.split_before(0.8)\n",
    "        time_idx = val.time_index\n",
    "        pred = forecasts[best_model]\n",
    "        val_df  = val.to_dataframe()  \n",
    "        pred_df = pred.to_dataframe()\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        val.plot(label=\"Observed\")\n",
    "        pred.plot(label=f\"Forecast - {best_model}\")\n",
    "        plt.title(f\"Validation vs Prediction â€” {st} | {ts}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(model_folder, f\"val_vs_pred_{st}_{ts}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"âœ”ï¸ Done with {st} | {ts}\\n\")\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    # Identify best model (custom score and RMSE)\n",
    "    df_summary['Best_Model'] = df_summary.groupby(['Station', 'Timescale'])['Score'].transform(\n",
    "        lambda x: x == x.min()\n",
    "    )\n",
    "    df_summary['Best_RMSE'] = df_summary.groupby(['Station', 'Timescale'])['RMSE'].transform(\n",
    "        lambda x: x == x.min()\n",
    "    )\n",
    "    df_summary['Winner'] = df_summary.apply(\n",
    "    lambda row: 'âœ”ï¸' if row['Best_Model'] else '', axis=1\n",
    ")\n",
    "\n",
    "    # Save CSV\n",
    "    df_summary_path = os.path.join(base_dir, 'results_summary.csv')\n",
    "    df_summary.to_csv(df_summary_path, index=False)\n",
    "    print(f\"ðŸ“„ Results saved as CSV: {df_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "435d6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 50.7 K | train\n",
      "6 | V               | Linear           | 65     | train\n",
      "-------------------------------------------------------------\n",
      "50.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "50.8 K    Total params\n",
      "0.203     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 112.85it/s, train_loss=0.0151] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 112.45it/s, train_loss=0.0151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ValueError: For the given forecasting case, the provided future covariates at dataset index `0` do not extend far enough into the past. The future covariates must start at time step `2022-05-01 00:00:00`, whereas now they start at time step `2024-05-01 00:00:00`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For the given forecasting case, the provided future covariates at dataset index `0` do not extend far enough into the past. The future covariates must start at time step `2022-05-01 00:00:00`, whereas now they start at time step `2024-05-01 00:00:00`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m horizon      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mdate_range(target\u001b[38;5;241m.\u001b[39mend_time()\u001b[38;5;241m+\u001b[39mMonthEnd(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2050-12-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# let Darts handle the spi_lag_* features automatically\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m future_spi \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_cov\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# append & save\u001b[39;00m\n\u001b[1;32m     47\u001b[0m full \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mappend(future_spi)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/utils/torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/models/forecasting/torch_forecasting_model.py:1530\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict\u001b[0;34m(self, n, series, past_covariates, future_covariates, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters, show_warnings)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m   1512\u001b[0m     n,\n\u001b[1;32m   1513\u001b[0m     series,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m     show_warnings\u001b[38;5;241m=\u001b[39mshow_warnings,\n\u001b[1;32m   1519\u001b[0m )\n\u001b[1;32m   1521\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_inference_dataset(\n\u001b[1;32m   1522\u001b[0m     target\u001b[38;5;241m=\u001b[39mseries,\n\u001b[1;32m   1523\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1528\u001b[0m )\n\u001b[0;32m-> 1530\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroll_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroll_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmc_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmc_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_likelihood_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m called_with_single_series \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/utils/torch.py:80\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m     79\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/models/forecasting/torch_forecasting_model.py:1623\u001b[0m, in \u001b[0;36mTorchForecastingModel.predict_from_dataset\u001b[0;34m(self, n, input_series_dataset, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters)\u001b[0m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_inference_dataset_type(input_series_dataset)\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;66;03m# check that covariates and dimensions are matching what we had during training\u001b[39;00m\n\u001b[0;32m-> 1623\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_predict_sample(\u001b[43minput_series_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m roll_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1626\u001b[0m     roll_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_chunk_length\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/utils/data/inference_dataset.py:600\u001b[0m, in \u001b[0;36mDualCovariatesInferenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m, idx\n\u001b[1;32m    585\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     Union[pd\u001b[38;5;241m.\u001b[39mTimestamp, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    592\u001b[0m ]:\n\u001b[1;32m    593\u001b[0m     (\n\u001b[1;32m    594\u001b[0m         past_target,\n\u001b[1;32m    595\u001b[0m         historic_future_covariate,\n\u001b[1;32m    596\u001b[0m         _,\n\u001b[1;32m    597\u001b[0m         static_covariate,\n\u001b[1;32m    598\u001b[0m         ts_target,\n\u001b[1;32m    599\u001b[0m         pred_point,\n\u001b[0;32m--> 600\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds_past\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    601\u001b[0m     _, future_covariate, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_future[idx]\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    603\u001b[0m         past_target,\n\u001b[1;32m    604\u001b[0m         historic_future_covariate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         pred_point,\n\u001b[1;32m    609\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/utils/data/inference_dataset.py:419\u001b[0m, in \u001b[0;36mPastCovariatesInferenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m    411\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     Union[pd\u001b[38;5;241m.\u001b[39mTimestamp, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    418\u001b[0m ]:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/utils/data/inference_dataset.py:292\u001b[0m, in \u001b[0;36mGenericInferenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    287\u001b[0m covariate_series \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariates[series_idx]\n\u001b[1;32m    289\u001b[0m )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m covariate_series \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# get start and end indices (integer) of the covariates including historic and future parts\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     covariate_start, covariate_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_covariate_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseries_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_start_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovariate_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcovariate_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcovariate_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariate_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_chunk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_chunk_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_chunk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_chunk_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_chunk_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_chunk_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# extract covariate values and split into a past (historic) and future part\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     covariate \u001b[38;5;241m=\u001b[39m covariate_series\u001b[38;5;241m.\u001b[39mrandom_component_values(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\n\u001b[1;32m    306\u001b[0m         covariate_start:covariate_end\n\u001b[1;32m    307\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/utils/data/inference_dataset.py:99\u001b[0m, in \u001b[0;36mInferenceDataset._covariate_indexer\u001b[0;34m(target_idx, past_start, past_end, covariate_series, covariate_type, input_chunk_length, output_chunk_length, output_chunk_shift, n)\u001b[0m\n\u001b[1;32m     95\u001b[0m case_start \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     96\u001b[0m     future_start \u001b[38;5;28;01mif\u001b[39;00m covariate_type \u001b[38;5;129;01mis\u001b[39;00m CovariateType\u001b[38;5;241m.\u001b[39mFUTURE \u001b[38;5;28;01melse\u001b[39;00m past_start\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m covariate_series\u001b[38;5;241m.\u001b[39mstart_time() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m case_start:\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mraise_log\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFor the given forecasting case, the provided \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmain_covariate_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m covariates at \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset index `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m` do not extend far enough into the past. The \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmain_covariate_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m covariates must start at time step `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcase_start\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m`, whereas now \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthey start at time step `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcovariate_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m`.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m covariate_series\u001b[38;5;241m.\u001b[39mend_time() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m future_end:\n\u001b[1;32m    109\u001b[0m     raise_log(\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor the given forecasting horizon `n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, the provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_covariate_type\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m covariates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m    118\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/darts/logging.py:132\u001b[0m, in \u001b[0;36mraise_log\u001b[0;34m(exception, logger)\u001b[0m\n\u001b[1;32m    129\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exception)\n\u001b[1;32m    130\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(exception_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: For the given forecasting case, the provided future covariates at dataset index `0` do not extend far enough into the past. The future covariates must start at time step `2022-05-01 00:00:00`, whereas now they start at time step `2024-05-01 00:00:00`."
     ]
    }
   ],
   "source": [
    "# Cell 4 â€” Forecast to 2050 refit on full history, Dartsâ€native lags\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "def build_future_covariates(covariates: TimeSeries, end_date: str) -> TimeSeries:\n",
    "    df = covariates.to_dataframe()[['rainfall']]      # only rainfall\n",
    "    df['month'] = df.index.month\n",
    "    monthly_means = df.groupby('month').mean()['rainfall']\n",
    "\n",
    "    last_hist  = covariates.end_time()\n",
    "    future_idx = pd.date_range(\n",
    "        start=last_hist + MonthEnd(0),\n",
    "        end=end_date,\n",
    "        freq=\"MS\"\n",
    "    )\n",
    "    df_future = pd.DataFrame(index=future_idx)\n",
    "    df_future['rainfall'] = df_future.index.month.map(monthly_means)\n",
    "    return TimeSeries.from_dataframe(df_future.astype(np.float32))\n",
    "\n",
    "\n",
    "for st in [40700]:\n",
    "    for ts in [1,3,6]:\n",
    "        data   = make_features(df[df['station']==st], ts)\n",
    "        target = TimeSeries.from_series(data[\"spi_deseason\"]).astype(np.float32)\n",
    "        # here covariates only rainfallâ€”the spi_lag_* are generated by Darts\n",
    "        cov    = TimeSeries.from_dataframe(data[['rainfall']]).astype(np.float32)\n",
    "        target, cov = target.slice_intersect(cov), cov.slice_intersect(target)\n",
    "\n",
    "        # pick & retrain best on full history\n",
    "        df_sum = pd.read_csv(os.path.join(base_dir,'results_summary.csv'))\n",
    "        best   = df_sum.query(\"Station==@st and Timescale==@ts and Best_Model\")[ 'Model' ].iloc[0]\n",
    "        model  = model_constructors[best]()\n",
    "        model.fit(series=target, future_covariates=cov)\n",
    "\n",
    "        # build future covariates (rainfall only)\n",
    "        future_cov   = build_future_covariates(cov, \"2050-12-01\")\n",
    "        horizon      = len(pd.date_range(target.end_time()+MonthEnd(0), \"2050-12-01\", freq=\"MS\"))\n",
    "\n",
    "        # let Darts handle the spi_lag_* features automatically\n",
    "        future_spi = model.predict(\n",
    "            horizon,\n",
    "            series=target,\n",
    "            future_covariates=future_cov\n",
    "        )\n",
    "\n",
    "        # append & save\n",
    "        full = target.append(future_spi)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        full.plot(label=\"SPI\")\n",
    "        plt.axvline(target.end_time(), color='r', ls='--', label=\"Forecast Start\")\n",
    "        plt.title(f\"Station {st} â€” SPI-{ts} Forecast with {best}\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir,f\"{st}/{ts}/Forecast_to_2050.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        full.to_dataframe().reset_index().rename(columns={'index':'date',0:'spi_deseason'})\\\n",
    "            .to_csv(os.path.join(base_dir,f\"{st}/{ts}/SPI_forecast_2050.csv\"), index=False)\n",
    "        print(f\" â€¢ Forecast + CSV saved for SPI-{ts} ({best})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489fa27",
   "metadata": {},
   "source": [
    "forecasting with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "#     # best.fit(series=target_series, future_covariates=covariates)\n",
    "        #     best.fit(series=target_series)\n",
    "\n",
    "        \n",
    "\n",
    "        # cov_df = covariates.pd_dataframe()  # or however you get a pandas DataFrame\n",
    "        # cov_df['month'] = cov_df.index.month\n",
    "\n",
    "        # # Compute monthly means for each column except 'month'\n",
    "        # monthly_means = cov_df.groupby('month').mean()\n",
    "\n",
    "        # # 2) Build the future date index from Mayâ€¯2024 to Aprilâ€¯2051\n",
    "        # last_hist = covariates.end_time()         # e.g. Timestamp('2024-04-01 00:00:00')\n",
    "        # start_future = last_hist + MonthEnd(1)    # gives end of month, but for MS freq it lines up\n",
    "        # future_idx = pd.date_range(start=start_future,\n",
    "        #                         end=\"2051-04-01\",\n",
    "        #                         freq=\"MS\")\n",
    "\n",
    "        # # 3) Create a DataFrame for future covariates by mapping each future month to its climatology\n",
    "        # df_future = pd.DataFrame(index=future_idx)\n",
    "\n",
    "        # # For each covariate column, fill with the corresponding monthly mean\n",
    "        # for col in monthly_means.columns:\n",
    "        #     df_future[col] = [monthly_means.loc[m, col] for m in df_future.index.month]\n",
    "\n",
    "        # # 4) Convert to a Darts TimeSeries\n",
    "        # full_df = pd.concat([covariates.pd_dataframe(), df_future])\n",
    "        # future_covariates = TimeSeries.from_dataframe(full_df)\n",
    "        # future_covariates = TimeSeries.from_dataframe(df_future)\n",
    "        # Forecast to 2050\n",
    "        # horizon = (pd.Timestamp(\"2050-12-01\") - target_series.end_time()).days // 30\n",
    "        # future = best.predict(horizon, series=target_series,future_covariates=future_covariates)\n",
    "        # future = best.predict(horizon, series=target_series)\n",
    "\n",
    "\n",
    "        # if not isinstance(future.time_index, pd.DatetimeIndex):\n",
    "        #     future = TimeSeries.from_times_and_values(\n",
    "        #         pd.date_range(\n",
    "        #             start=target_series.end_time() + pd.DateOffset(months=1),\n",
    "        #             periods=len(future),\n",
    "        #             freq=\"MS\"\n",
    "        #         ),\n",
    "        #         future.values(),\n",
    "        #         columns=target_series.components\n",
    "        #     )\n",
    "\n",
    "        # historical = target_series\n",
    "        # full_series = historical.append(future)\n",
    "\n",
    "        # plt.figure(figsize=(12, 4))\n",
    "        # full_series.plot(label=\"SPI\")\n",
    "        # plt.axvline(x=historical.end_time(), color='r', linestyle='--', label=\"Forecast Start\")\n",
    "        # plt.title(f\"Forecast 2050 {st} | {ts}: {best_model}\")\n",
    "\n",
    "        # plt.xlabel(\"Time\")\n",
    "        # plt.ylabel(\"SPI\")\n",
    "        # plt.grid(True)\n",
    "        # plt.legend()\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(os.path.join(model_folder, f\"{st}/Forecast_{ts}.png\"))\n",
    "        # plt.show()\n",
    "\n",
    "# spi_df = future.pd_dataframe().reset_index()\n",
    "        # spi_df['year'] = pd.to_datetime(spi_df['date']).dt.year\n",
    "        # spi_df['month'] = pd.to_datetime(spi_df['date']).dt.month\n",
    "\n",
    "        # # SPI heatmap\n",
    "        # heatmap_data = spi_df.pivot_table(index='year', columns='month', values=\"spi_deseason\")\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # sns.heatmap(heatmap_data, cmap='rocket', center=0, annot=True, fmt=\".2f\")\n",
    "        # plt.title(f\"SPI Heatmap â€” {ts} â€” {st}\")\n",
    "        # plt.xlabel(\"Month\")\n",
    "        # plt.ylabel(\"Year\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.grid(False)\n",
    "        # plt.savefig(os.path.join(base_dir, f\"{st}/heatmap_{ts}.png\"))\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # spi_df['category'] = pd.cut(spi_df[\"spi_deseason\"], bins=[-np.inf, -1, 1, np.inf], labels=['Dry', 'Normal', 'Wet'])\n",
    "        # colors = {'Dry': 'red', 'Normal': 'gray', 'Wet': 'blue'}\n",
    "\n",
    "        # plt.figure(figsize=(14, 6))\n",
    "        # for category, color in colors.items():\n",
    "        #     mask = spi_df['category'] == category\n",
    "        #     plt.scatter(spi_df['date'][mask], spi_df['spi_deseason'][mask], c=color, label=category)\n",
    "\n",
    "        # plt.axhline(0, color='black', lw=1, linestyle='--')\n",
    "        # plt.title(f\"SPI Categories: Dry / Normal / Wet â€” {st} | {ts}\")\n",
    "        # plt.xlabel(\"Date\")\n",
    "        # plt.ylabel(\"SPI Value\")\n",
    "        # plt.grid(True)\n",
    "        # plt.tight_layout()\n",
    "        # plt.legend()\n",
    "        # plt.savefig(os.path.join(base_dir, f\"{st}/scatter_{ts}.png\"))\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # # Use a simple threshold detector\n",
    "        # detector = ThresholdAD(low_threshold=-1.5, high_threshold=1.5)\n",
    "        # anomalies = detector.detect(full_series)\n",
    "\n",
    "        # # Plot\n",
    "        # full_series.plot(label=\"SPI\")\n",
    "        # anomalies.plot(label=\"Anomalies\", color='red', marker='o')\n",
    "        # plt.legend()\n",
    "        # plt.title(f\"Darts Anomaly Detection â€” {station_id} | {col}\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a3e1",
   "metadata": {},
   "source": [
    "Loop over stations & timescales, \n",
    " forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf9ee0",
   "metadata": {},
   "source": [
    "then auto slide creation or pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_path = os.path.join(base_dir, \"SPI_Results_Summary.pptx\")\n",
    "prs = Presentation()\n",
    "\n",
    "title_slide_layout = prs.slide_layouts[0]\n",
    "blank_slide_layout = prs.slide_layouts[6]\n",
    "\n",
    "# Title slide\n",
    "slide = prs.slides.add_slide(title_slide_layout)\n",
    "slide.shapes.title.text = \"SPI Forecast & Evaluation Summary\"\n",
    "# slide.placeholders[1].text = \"Auto-generated using python-pptx\\nIncludes Taylor Diagrams, Heatmaps, Model Metrics & Forecasts\"\n",
    "\n",
    "# image_summaries = {\n",
    "#     \"val_vs_pred\": \"Comparison of predicted vs. actual SPI values. Good alignment indicates accurate forecasting.\",\n",
    "#     \"heatmap\": \"Heatmap of forecast performance over time. Brighter regions indicate higher error or uncertainty.\",\n",
    "#     \"taylor\": \"Taylor diagram summarizing model skill. Closer proximity to reference indicates better performance.\",\n",
    "#     \"scatter\": \"Scatter plot of predicted vs. observed SPI. Closer points to diagonal line show better predictions.\"\n",
    "# }\n",
    "\n",
    "def generate_summary(img_type, model_metrics):\n",
    "    \"\"\"Create a smart summary based on image type and model metrics.\"\"\"\n",
    "    if not model_metrics:\n",
    "        return \"Performance data not available.\"\n",
    "\n",
    "    # Pick the best model based on RMSE (you can use another metric too)\n",
    "    best_model, (std_o, std_p, corr, rmse, mape,sm) = min(model_metrics.items(), key=lambda x: x[1][3])  # sort by RMSE\n",
    "\n",
    "    # Interpret performance\n",
    "    if rmse < 0.3 and mape < 10 and corr > 0.85:\n",
    "        perf = \"excellent\"\n",
    "    elif rmse < 0.6 and mape < 20 and corr > 0.65:\n",
    "        perf = \"reasonable\"\n",
    "    else:\n",
    "        perf = \"poor\"\n",
    "\n",
    "    # Now create summaries\n",
    "    if img_type == \"val_vs_pred\":\n",
    "        return f\"Predicted vs. observed SPI using {best_model}. Alignment is {perf}, with RMSE={rmse:.2f}, Corr={corr:.2f}.\"\n",
    "    elif img_type == \"heatmap\":\n",
    "        return f\"Heatmap of error over time for {best_model}. Performance is {perf}, with average MAPE={mape:.1f}%.\"\n",
    "    elif img_type == \"taylor\":\n",
    "        return f\"Taylor diagram showing model spread vs. observed. {best_model} shows {perf} alignment with reference point.\"\n",
    "    elif img_type == \"scatter\":\n",
    "        return f\"Scatter plot for {best_model}. {perf.capitalize()} correlation between predictions and observations (Corr={corr:.2f}).\"\n",
    "    else:\n",
    "        return \"Performance visualization.\"\n",
    "\n",
    "# Loop through all stations\n",
    "for station_id in sorted(os.listdir(base_dir)):\n",
    "    station_path = os.path.join(base_dir, station_id)\n",
    "    if not os.path.isdir(station_path):\n",
    "        continue\n",
    "\n",
    "    for col in results.get(int(station_id), {}):  # Ensure `station_id` matches results keys\n",
    "        # Add slide for this station/SPI\n",
    "        slide = prs.slides.add_slide(blank_slide_layout)\n",
    "        # Add title\n",
    "        title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(0.5))\n",
    "        tf = title_box.text_frame\n",
    "        tf.text = f\"Station {station_id} â€” {col}\"\n",
    "        tf.paragraphs[0].font.size = Pt(24)\n",
    "        tf.paragraphs[0].font.bold = True\n",
    "\n",
    "        # Add metrics table\n",
    "        metrics = results[int(station_id)][col]\n",
    "        rows, cols = len(metrics) + 1, 6\n",
    "        table = slide.shapes.add_table(rows, cols, Inches(0.5), Inches(1), Inches(8), Inches(0.6 + rows * 0.4)).table\n",
    "        table.cell(0, 0).text = \"Model\"\n",
    "        table.cell(0, 1).text = \"RMSE\"\n",
    "        table.cell(0, 2).text = \"MAPE\"\n",
    "        table.cell(0, 3).text = \"Corr\"\n",
    "        table.cell(0, 4).text = \"Std. Dev (Pred)\"\n",
    "        table.cell(0, 5).text = \"smape\"\n",
    "\n",
    "        for i, (model_name, (std_o, std_p, corr, rmse, mape,sm)) in enumerate(metrics.items(), start=1):\n",
    "            table.cell(i, 0).text = model_name\n",
    "            table.cell(i, 1).text = f\"{rmse:.3f}\"\n",
    "            table.cell(i, 2).text = f\"{mape:.2f}%\"\n",
    "            table.cell(i, 3).text = f\"{corr:.2f}\"\n",
    "            table.cell(i, 4).text = f\"{std_p:.2f}\"\n",
    "            table.cell(i, 5).text = f\"{sm:.2f}\"\n",
    "\n",
    "        # Add first two images\n",
    "        # image_files = [\"taylor\",\"val_vs_pred\",  \"heatmap\", \"scatter\"]\n",
    "        image_files = [\"taylor\",\"val_vs_pred\",\"Forecast\", \"heatmap\",  \"scatter\"]\n",
    "        img_slide_count = 0\n",
    "        img_group = []\n",
    "\n",
    "        for img_type in image_files:\n",
    "            img_path = os.path.join(station_path, f\"{img_type}_{col}.png\")\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "\n",
    "            # Create a new slide for each image\n",
    "            current_slide = prs.slides.add_slide(blank_slide_layout)\n",
    "\n",
    "            # Add slide title\n",
    "            sub_title_box = current_slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(0.5))\n",
    "            sub_tf = sub_title_box.text_frame\n",
    "            sub_tf.text = f\"Station {station_id} â€” {col} ({img_type.replace('_', ' ').title()})\"\n",
    "            sub_tf.paragraphs[0].font.size = Pt(20)\n",
    "            sub_tf.paragraphs[0].font.bold = True\n",
    "\n",
    "            # Add image\n",
    "            x = Inches(0.5)\n",
    "            y_img = Inches(1.0)\n",
    "            current_slide.shapes.add_picture(img_path, x, y_img, width=Inches(8.5))  # Full width if needed\n",
    "\n",
    "            # Add summary text\n",
    "            y_text = y_img + Inches(5.8)\n",
    "            model_metrics = results[int(station_id)][col]\n",
    "            summary_text = generate_summary(img_type, model_metrics)\n",
    "\n",
    "            textbox = current_slide.shapes.add_textbox(x, y_text, width=Inches(8.5), height=Inches(1))\n",
    "            tf = textbox.text_frame\n",
    "            tf.text = summary_text\n",
    "            tf.paragraphs[0].font.size = Pt(12)\n",
    "\n",
    "        # for img_type in image_files:\n",
    "        #     img_path = os.path.join(station_path, f\"{img_type}_{col}.png\")\n",
    "        #     if os.path.exists(img_path):\n",
    "        #         img_group.append(img_path)\n",
    "\n",
    "        # # Split image group into chunks of 2\n",
    "        # for i in range(0, len(img_group), 2):\n",
    "        #     if i == 0:\n",
    "        #         # Use the first slide already created\n",
    "        #         current_slide = slide\n",
    "        #     else:\n",
    "        #         current_slide = prs.slides.add_slide(blank_slide_layout)\n",
    "\n",
    "        #         # Add title for extra image slides\n",
    "        #         sub_title_box = current_slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(0.5))\n",
    "        #         sub_tf = sub_title_box.text_frame\n",
    "        #         sub_tf.text = f\"Station {station_id} â€” {col} (Images {i + 1}â€“{min(i+2, len(img_group))})\"\n",
    "        #         sub_tf.paragraphs[0].font.size = Pt(20)\n",
    "        #         sub_tf.paragraphs[0].font.bold = True\n",
    "\n",
    "        #     for j, img_path in enumerate(img_group[i:i+2]):\n",
    "        #         x = Inches(0.5 + j * 5)  # Side by side\n",
    "        #         y_img  = Inches(2.7 if i == 0 else 1.0)\n",
    "        #         y_text = y_img + Inches(3.6)  # Position text below image\n",
    "        #         current_slide.shapes.add_picture(img_path, x, y_img, width=Inches(4.5))\n",
    "\n",
    "        #         # Extract image type to get its description\n",
    "        #         img_filename = os.path.basename(img_path)\n",
    "        #         img_type = img_filename.split(\"_\")[0]\n",
    "\n",
    "        #         model_metrics = results[int(station_id)][col]\n",
    "        #         summary_text = generate_summary(img_type, model_metrics)\n",
    "\n",
    "\n",
    "        #         textbox = current_slide.shapes.add_textbox(x, y_text, width=Inches(4.5), height=Inches(1))\n",
    "        #         tf = textbox.text_frame\n",
    "        #         tf.text = summary_text\n",
    "        #         tf.paragraphs[0].font.size = Pt(12)\n",
    "\n",
    "# Save presentation\n",
    "prs.save(ppt_path)\n",
    "print(f\"âœ… Presentation saved to: {ppt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680435b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "base_dir = \"all_results\"\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading(\"SPI Forecast & Evaluation Summary\", 0)\n",
    "spi_periods = [\"1\", \"3\", \"6\", \"9\", \"12\", \"24\"]\n",
    "\n",
    "\n",
    "def generate_summary(img_type, model_metrics):\n",
    "    if not model_metrics:\n",
    "        return None\n",
    "\n",
    "    best_model, (std_o, std_p, corr, rmse, mape, sm) = min(model_metrics.items(), key=lambda x: x[1][3])\n",
    "    if rmse < 0.3 and mape < 10 and corr > 0.85:\n",
    "        perf = \"excellent\"\n",
    "    elif rmse < 0.6 and mape < 20 and corr > 0.65:\n",
    "        perf = \"reasonable\"\n",
    "    else:\n",
    "        perf = \"poor\"\n",
    "\n",
    "    if img_type == \"val_vs_pred\":\n",
    "        return f\"{best_model}: RMSE={rmse:.2f}, Corr={corr:.2f} â€” {perf} performance.\"\n",
    "    elif img_type == \"heatmap\":\n",
    "        return f\"{best_model} temporal map: Avg. MAPE={mape:.1f}% â€” {perf} performance.\"\n",
    "    elif img_type == \"taylor\":\n",
    "        return f\"{best_model} Taylor diagram â€” {perf} skill alignment.\"\n",
    "    return None\n",
    "\n",
    "# Inputs\n",
    "image_types  = {\n",
    "    \"taylor\": \"Taylor Diagram\",\n",
    "    \"heatmap\": \"Temporal Map\",\n",
    "    \"val_vs_pred\": \"Validation vs. Prediction\"\n",
    "}\n",
    "\n",
    "# def add_image(doc, img_path, caption, width=Inches(2.9)):\n",
    "#     if not os.path.exists(img_path):\n",
    "#         return\n",
    "#     doc.add_picture(img_path, width=width)\n",
    "#     last_paragraph = doc.paragraphs[-1]\n",
    "#     last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "#     caption_paragraph = doc.add_paragraph(caption)\n",
    "#     caption_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "#     caption_paragraph.style.font.size = Pt(9)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def crop_bottom_whitespace(image_path, save_path=None, crop_ratio=0.3):\n",
    "    \"\"\"Crop the bottom portion of the image.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        cropped_img = img.crop((0, 0, width, int(height * (1 - crop_ratio))))\n",
    "        cropped_img.save(save_path or image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error cropping {image_path}: {e}\")\n",
    "\n",
    "# Loop through station folders\n",
    "for station_id in sorted(os.listdir(base_dir)):\n",
    "    station_path = os.path.join(base_dir, station_id)\n",
    "    if not os.path.isdir(station_path) or not station_id.isdigit():\n",
    "        continue\n",
    "\n",
    "    doc.add_heading(f\"Station {station_id}\", level=1)\n",
    "\n",
    "    for spi in spi_periods:\n",
    "        doc.add_heading(f\"SPI-{spi}\", level=2)\n",
    "\n",
    "        table = doc.add_table(rows=1, cols=3)\n",
    "        table.autofit = True\n",
    "        row = table.rows[0]\n",
    "\n",
    "        for i, prefix in enumerate([\"taylor\", \"heatmap\", \"val_vs_pred\"]):\n",
    "            file_name = f\"{prefix}_SPI_{spi}.png\"\n",
    "            img_path = os.path.join(station_path, file_name)\n",
    "\n",
    "            # Crop white space from Taylor diagram if needed\n",
    "            if prefix == \"taylor\" and os.path.exists(img_path):\n",
    "                crop_bottom_whitespace(img_path)\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                paragraph = row.cells[i].paragraphs[0]\n",
    "                run = paragraph.add_run()\n",
    "                run.add_picture(img_path, width=Inches(2.2))\n",
    "                row.cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "        doc.add_paragraph()  # slight space between rows\n",
    "\n",
    "    doc.add_page_break()\n",
    "\n",
    "doc_path = os.path.join(base_dir, \"SPI_Results_Summary.docx\")\n",
    "doc.save(doc_path)\n",
    "print(f\"âœ… Word document saved to: {doc_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb6bbd",
   "metadata": {},
   "source": [
    "Iran Stations Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8473505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch, Rectangle\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import Rbf\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# === Load shapefiles ===\n",
    "iran = gpd.read_file('materials/iran.shp')   \n",
    "sea = gpd.read_file('materials/seas.shp') \n",
    "\n",
    "# === Load and prepare station data ===\n",
    "stations_df = pd.read_csv('../main_data.csv')\n",
    "stations = stations_df[['station_id', 'station_name', 'station_elevation', 'lat', 'lon']].drop_duplicates()\n",
    "geometry = [Point(xy) for xy in zip(stations['lon'], stations['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(stations, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "buffer = 0.3  # Optional: adds margin around stations\n",
    "xmin, xmax = geo_df.geometry.x.min() - buffer, geo_df.geometry.x.max() + buffer\n",
    "ymin, ymax = geo_df.geometry.y.min() - buffer, geo_df.geometry.y.max() + buffer\n",
    "\n",
    "\n",
    "# === Interpolate elevation ===\n",
    "x = geo_df.geometry.x.values\n",
    "y = geo_df.geometry.y.values\n",
    "z = geo_df['station_elevation'].values\n",
    "grid_res = 500\n",
    "# xi = np.linspace(iran.total_bounds[0], iran.total_bounds[2], grid_res)\n",
    "# yi = np.linspace(iran.total_bounds[1], iran.total_bounds[3], grid_res)\n",
    "xi = np.linspace(xmin, xmax, grid_res)\n",
    "yi = np.linspace(ymin, ymax, grid_res)\n",
    "xi, yi = np.meshgrid(xi, yi)\n",
    "rbf = Rbf(x, y, z, function='linear')\n",
    "zi = rbf(xi, yi)\n",
    "\n",
    "# Create a mask from the Iran polygon\n",
    "iran_shape = iran.unary_union  # In case it's a multi-polygon\n",
    "mask = np.ones_like(zi, dtype=bool)\n",
    "\n",
    "# Build a Path object for masking\n",
    "# iran_path = Path(iran_shape.exterior.coords[:])\n",
    "\n",
    "# Go through each grid point and mask if it's outside\n",
    "for i in range(zi.shape[0]):\n",
    "    for j in range(zi.shape[1]):\n",
    "        point = (xi[0, j], yi[i, 0])  # Get X, Y of this pixel\n",
    "        if iran_shape.contains(Point(point)):\n",
    "            mask[i, j] = False\n",
    "\n",
    "# Apply mask to elevation\n",
    "zi_masked = np.ma.masked_array(zi, mask=mask)\n",
    "\n",
    "# === Define zoom region ===\n",
    "# # Set the zoom box manually for a region (e.g., Ardabil or any other)\n",
    "# zoom_xmin, zoom_xmax = 46.5, 49.5\n",
    "# zoom_ymin, zoom_ymax = 37.0, 39.0\n",
    "\n",
    "# === Setup figure ===\n",
    "fig = plt.figure(figsize=(5, 8))\n",
    "\n",
    "# --- Left: Full map of Iran ---\n",
    "ax_iran = fig.add_axes([0.02, 0.65, 0.5, 0.5])  # Top-left corner\n",
    "sea.plot(ax=ax_iran, color='lightblue')\n",
    "iran.plot(ax=ax_iran, color='lightgray', edgecolor='black')\n",
    "ax_iran.set_xticks([]); ax_iran.set_yticks([]); ax_iran.set_title('Iran Overview', fontsize=12)\n",
    "\n",
    "# # Draw zoom rectangle on Iran map\n",
    "# rect = Rectangle((zoom_xmin, zoom_ymin), zoom_xmax - zoom_xmin, zoom_ymax - zoom_ymin,\n",
    "#                  linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "# ax_iran.add_patch(rect)\n",
    "\n",
    "ax_iran.plot([xmin, xmax, xmax, xmin, xmin],\n",
    "             [ymin, ymin, ymax, ymax, ymin],\n",
    "             color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# --- Right: Zoomed-in elevation with stations ---\n",
    "ax_zoom = fig.add_axes([0.02, 0.05, 0.8, 0.8])  # Right side\n",
    "\n",
    "iran.plot(ax=ax_zoom, facecolor='none', edgecolor='black', linewidth=1.2)\n",
    "\n",
    "elev_img = ax_zoom.imshow(zi_masked, extent=(xi.min(), xi.max(), yi.min(), yi.max()),\n",
    "                          origin='lower', cmap='terrain', alpha=0.8)\n",
    "\n",
    "# # Set zoom limits\n",
    "# ax_zoom.set_xlim(zoom_xmin, zoom_xmax)\n",
    "# ax_zoom.set_ylim(zoom_ymin, zoom_ymax)\n",
    "\n",
    "geo_df.plot(ax=ax_zoom, color='black', markersize=20, marker='^', zorder=3)\n",
    "\n",
    "for idx, row in geo_df.iterrows():\n",
    "    ax_zoom.text(row.geometry.x + 0.05, row.geometry.y, \n",
    "                 str(row['station_id']), fontsize=8, color='black')\n",
    "# ax_zoom.set_title('Zoomed Region: Elevation & Stations', fontsize=12)\n",
    "# ax_zoom.set_xticks([]); ax_zoom.set_yticks([])\n",
    "ax_zoom.set_xlim(xmin, xmax)\n",
    "ax_zoom.set_ylim(ymin, ymax)\n",
    "# ax_zoom.set_xticks([]); ax_zoom.set_yticks([])\n",
    "\n",
    "# --- Colorbar ---\n",
    "cbar = fig.colorbar(elev_img, ax=ax_zoom, orientation='vertical', pad=0.09, fraction=0.03)\n",
    "# cbar.set_label('Elevation (m)')\n",
    "\n",
    "# --- Compass Rose (top right of zoom view) ---\n",
    "compass_img = mpimg.imread('materials/vecteezy_nautical-compass-icon_.jpg')\n",
    "ax_compass = fig.add_axes([0.68, 0.77, 0.2, 0.2])  # Adjust position/size here\n",
    "ax_compass.imshow(compass_img)\n",
    "ax_compass.axis('off')  # Hide axes frame\n",
    "ax_compass.set_title(\"N\", fontsize=10, pad=-10)\n",
    "# imagebox = OffsetImage(compass_img, zoom=0.06)\n",
    "# ab = AnnotationBbox(imagebox, (0.91, 0.91), xycoords='axes fraction', frameon=False,)\n",
    "# ax_zoom.add_artist(ab)\n",
    "\n",
    "# --- Add lines from Iran map to zoomed region ---\n",
    "# Calculate rectangle corners\n",
    "corner1 = (xmin, ymin)\n",
    "corner2 = (xmax, ymax)\n",
    "\n",
    "# Line from bottom-left corner of zoom box\n",
    "con1 = ConnectionPatch(xyA=corner1, xyB=corner1, coordsA=\"data\", coordsB=\"data\",\n",
    "                       axesA=ax_iran, axesB=ax_zoom, color=\"red\", linestyle='--')\n",
    "fig.add_artist(con1)\n",
    "\n",
    "# Line from top-right corner of zoom box\n",
    "con2 = ConnectionPatch(xyA=corner2, xyB=corner2, coordsA=\"data\", coordsB=\"data\",\n",
    "                       axesA=ax_iran, axesB=ax_zoom, color=\"red\", linestyle='--')\n",
    "fig.add_artist(con2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"iran_stations.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e069888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../main_data.csv\")\n",
    "\n",
    "# Group by station and calculate averages\n",
    "summary = df.groupby(['station_id', 'station_name', 'region_name', 'station_elevation'])[\n",
    "    ['tm_m', 'tmax_m', 'tmin_m', 'rrr24']\n",
    "].mean().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary.columns = [\n",
    "    'Station ID', 'Station Name', 'Region', 'Elevation (m)',\n",
    "    'Avg Temp (Â°C)', 'Avg Max Temp (Â°C)', 'Avg Min Temp (Â°C)', 'Avg Rainfall (mm/month)'\n",
    "]\n",
    "\n",
    "# Optional: Export to Excel or Word\n",
    "summary.to_excel(\"climate_summary.xlsx\", index=False)\n",
    "\n",
    "# Or, export to Word using python-docx (optional)\n",
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading('Climate Summary of Stations', 0)\n",
    "\n",
    "table = doc.add_table(rows=1, cols=len(summary.columns))\n",
    "table.style = 'Table Grid'\n",
    "\n",
    "# Add headers\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, column in enumerate(summary.columns):\n",
    "    hdr_cells[i].text = column\n",
    "\n",
    "# Add data rows\n",
    "for _, row in summary.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    for i, item in enumerate(row):\n",
    "        row_cells[i].text = str(round(item, 2)) if isinstance(item, float) else str(item)\n",
    "\n",
    "doc.save(\"climate_summary.docx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
