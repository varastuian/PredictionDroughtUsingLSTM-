{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib seaborn numpy scipy geopandas shapely xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import HandleData as HandleData\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import gamma, norm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.stats import gamma, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\PredictionDroughtUsingLSTM-\\Codes\\HandleData.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['data'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mHandleData\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mhd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m hd\u001b[38;5;241m.\u001b[39mby_station(\u001b[38;5;241m40708\u001b[39m)\n",
      "File \u001b[1;32md:\\Documents\\PredictionDroughtUsingLSTM-\\Codes\\HandleData.py:9\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspi_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6293\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6297\u001b[0m ):\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py:643\u001b[0m, in \u001b[0;36mCombinedDatetimelikeProperties.__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, PeriodDtype):\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "import HandleData as hd\n",
    "\n",
    "df = hd.get_data()\n",
    "hd.by_station(40708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_data.csv\")\n",
    "df['data'] = pd.to_datetime(df['data'])\n",
    "\n",
    "\n",
    "station_df = df[df['station_name'] == 40708].copy()\n",
    "\n",
    "# Create separate year and month columns for aggregation\n",
    "station_df['year'] = station_df['data'].dt.year\n",
    "station_df['month'] = station_df['data'].dt.month\n",
    "\n",
    "# Aggregate to monthly precipitation totals using the 'rrr24' column (assumed precipitation)\n",
    "monthly_precip = station_df.groupby(['year', 'month'])['rrr24'].sum().reset_index()\n",
    "\n",
    "# Extract precipitation values from the monthly data\n",
    "precip = monthly_precip['rrr24'].values\n",
    "\n",
    "# Calculate the probability of zero precipitation\n",
    "zero_prob = np.mean(precip == 0)\n",
    "\n",
    "# Extract only the non-zero precipitation values for fitting the gamma distribution\n",
    "nonzero_precip = precip[precip > 0]\n",
    "\n",
    "# Fit a gamma distribution to the nonzero precipitation values (fixing the location parameter to 0)\n",
    "fit_alpha, fit_loc, fit_beta = gamma.fit(nonzero_precip, floc=0)\n",
    "\n",
    "# Define a function to compute SPI for a given precipitation value\n",
    "def compute_spi(x):\n",
    "    if x == 0:\n",
    "        # Include the probability of zero precipitation\n",
    "        prob = zero_prob\n",
    "    else:\n",
    "        prob = zero_prob + (1 - zero_prob) * gamma.cdf(x, a=fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "    # Transform cumulative probability to a z-score (SPI value)\n",
    "    return norm.ppf(prob)\n",
    "\n",
    "# Apply the function to calculate SPI for each monthly total\n",
    "monthly_precip['SPI'] = monthly_precip['rrr24'].apply(compute_spi)\n",
    "\n",
    "# Display the resulting SPI values along with the corresponding year and month\n",
    "print(monthly_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import functions from the climate_indices package\n",
    "from climate_indices.compute import scale_values, Periodicity\n",
    "from climate_indices import indices, compute, utils\n",
    "\n",
    "# --- Step 1. Read CSV, Parse Dates, and Sort the Data ---\n",
    "# Update the file path as needed.\n",
    "# Here, we parse the 'data' column as datetime and sort by station (ion_name) and date.\n",
    "df = pd.read_csv(\"your_station_data.csv\", parse_dates=['data'])\n",
    "\n",
    "# If needed, rename the station id column (if \"ion_name\" is actually station_id)\n",
    "# df = df.rename(columns={'ion_name': 'station_id'})\n",
    "\n",
    "# Sort the DataFrame by station and date so that the time series are in chronological order.\n",
    "df = df.sort_values(by=['ion_name', 'data'])\n",
    "\n",
    "# Verify the sorting (optional)\n",
    "print(df.head())\n",
    "\n",
    "# --- Step 2. Pivot the DataFrame to Create a Time Series for Each Station ---\n",
    "# We pivot the data so that the index is the date and each column corresponds to a station's precipitation.\n",
    "# In this example, we use the 'rrr24' column as precipitation.\n",
    "df_pivot = df.pivot(index='data', columns='ion_name', values='rrr24')\n",
    "\n",
    "# Convert the pivoted DataFrame to an xarray DataArray.\n",
    "# The resulting DataArray 'da_precip' has dimensions: time x ion_name.\n",
    "da_precip = df_pivot.to_xarray()['rrr24']\n",
    "\n",
    "# --- Step 3. Define a Custom SPI Function ---\n",
    "# This SPI function is adapted to work on a 1-D array (a time series) of precipitation values.\n",
    "def spi(values, scale, distribution, data_start_year, calibration_year_initial, calibration_year_final, periodicity):\n",
    "    # If values come as 2-D, flatten them; otherwise ensure they are 1-D.\n",
    "    if values.ndim == 2:\n",
    "        values = values.flatten()\n",
    "    elif values.ndim != 1:\n",
    "        raise ValueError(f\"Invalid input shape {values.shape} – only 1-D or 2-D arrays are supported.\")\n",
    "    \n",
    "    # If the series is entirely missing, return it as is.\n",
    "    if np.all(np.isnan(values)):\n",
    "        return values\n",
    "    \n",
    "    # Ensure no negative values are used.\n",
    "    if np.amin(values) < 0:\n",
    "        values = np.clip(values, a_min=0.0, a_max=None)\n",
    "    \n",
    "    original_length = values.size\n",
    "    \n",
    "    # Scale the time series using a sliding sum over 'scale' time steps.\n",
    "    values = compute.sum_to_scale(values, scale)\n",
    "    \n",
    "    # Reshape into (years, months) for monthly data.\n",
    "    if periodicity is compute.Periodicity.monthly:\n",
    "        values = utils.reshape_to_2d(values, 12)\n",
    "    elif periodicity is compute.Periodicity.daily:\n",
    "        values = utils.reshape_to_2d(values, 366)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid periodicity argument\")\n",
    "    \n",
    "    # Fit the scaled data to a gamma distribution and transform to SPI values.\n",
    "    if distribution is indices.Distribution.gamma:\n",
    "        values = compute.transform_fitted_gamma(\n",
    "            values, data_start_year, calibration_year_initial, calibration_year_final, periodicity\n",
    "        )\n",
    "    elif distribution is indices.Distribution.pearson:\n",
    "        values = compute.transform_fitted_pearson(\n",
    "            values, data_start_year, calibration_year_initial, calibration_year_final, periodicity\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distribution argument\")\n",
    "    \n",
    "    # Clip the SPI values to the valid range.\n",
    "    fitted_min, fitted_max = -3.09, 3.09\n",
    "    values = np.clip(values, fitted_min, fitted_max).flatten()\n",
    "    \n",
    "    return values[:original_length]\n",
    "\n",
    "# --- Step 4. Set Up SPI Calculation Parameters ---\n",
    "# Determine the data start year from the time coordinate.\n",
    "initial_year = int(da_precip.time.dt.year.values[0])\n",
    "scale_months = 3  # using a 3-month accumulation\n",
    "\n",
    "# Adjust the calibration period as needed (here we use 1990–2010 as an example).\n",
    "spi_args = {\n",
    "    \"scale\": scale_months,\n",
    "    \"distribution\": indices.Distribution.gamma,\n",
    "    \"data_start_year\": initial_year,\n",
    "    \"calibration_year_initial\": 1990,\n",
    "    \"calibration_year_final\": 2010,\n",
    "    \"periodicity\": compute.Periodicity.monthly,\n",
    "}\n",
    "\n",
    "# --- Step 5. Compute SPI for Each Station ---\n",
    "# Apply the SPI function along the 'time' dimension for each station.\n",
    "# We use vectorize=True so that the function is applied to each station's time series independently.\n",
    "da_spi = xr.apply_ufunc(\n",
    "    spi,\n",
    "    da_precip,\n",
    "    input_core_dims=[[\"time\"]],\n",
    "    kwargs=spi_args,\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\"  # Remove or adjust if not using Dask\n",
    ")\n",
    "\n",
    "# --- Step 6. Plot the SPI for a Sample Station ---\n",
    "# For example, plot SPI for station with ion_name 40708.\n",
    "da_spi.sel(ion_name=40708).plot()\n",
    "plt.title(\"SPI (3-Month Scale) for Station 40708\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"SPI\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = HandleData.get_data()\n",
    "# HandleData.show_data()\n",
    "HandleData.stationIs(40708)\n",
    "exit()\n",
    "station_df = df[df['station_id'] == 40708]\n",
    "january_df = station_df[station_df['date'].dt.month == [1,3,5,7,9]]\n",
    "# january_df.head()\n",
    "# # View the entire DataFrame\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(january_df['date'], january_df['SPI'], label='SPI', color='blue')\n",
    "plt.title('SPI Time Series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('SPI')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "world = gpd.read_file('ir_shp\\ir.shp')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "world.plot(ax=ax, color='lightgray')\n",
    "\n",
    "# Extract unique station locations\n",
    "stations = df[['station_name', 'lat', 'lon']].drop_duplicates()\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(stations['lon'], stations['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(stations, geometry=geometry)\n",
    "\n",
    "# Set Coordinate Reference System (CRS) to WGS84 (EPSG:4326)\n",
    "geo_df.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Add station points\n",
    "geo_df.plot(ax=ax, color='red', markersize=50, label='Stations')\n",
    "\n",
    "# Add labels for station IDs\n",
    "for x, y, label in zip(geo_df.geometry.x, geo_df.geometry.y, geo_df['station_name']):\n",
    "    ax.text(x, y, str(label), fontsize=9, ha='right', color='blue')\n",
    "\n",
    "xmin, xmax = 44, 51  # Adjust longitude range\n",
    "ymin, ymax = 35, 40  # Adjust latitude range\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Station Locations', fontsize=16)\n",
    "plt.xlabel('Longitude', fontsize=12)\n",
    "plt.ylabel('Latitude', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# Load Iran shapefile\n",
    "world = gpd.read_file('ir_shp/ir.shp')\n",
    "\n",
    "\n",
    "# Extract unique station locations\n",
    "stations = df[['station_id', 'lat', 'lon']].drop_duplicates()\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(stations['lon'], stations['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(stations, geometry=geometry)\n",
    "\n",
    "# Set Coordinate Reference System (CRS) to WGS84 (EPSG:4326)\n",
    "geo_df.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Plot the full map\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "world.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "geo_df.plot(ax=ax, color='red', markersize=50, label='Stations')\n",
    "\n",
    "# Add labels for station IDs\n",
    "for x, y, label in zip(geo_df.geometry.x, geo_df.geometry.y, geo_df['station_id']):\n",
    "    ax.text(x, y, str(label), fontsize=9, ha='right', color='blue')\n",
    "\n",
    "# Customize the main plot\n",
    "plt.title('Station Locations (Full Map and Zoomed-In View)', fontsize=16)\n",
    "plt.xlabel('Longitude', fontsize=12)\n",
    "plt.ylabel('Latitude', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Create an inset for the zoomed-in view\n",
    "axins = inset_axes(ax, width=\"40%\", height=\"40%\", loc='upper right')  # Adjust size and location\n",
    "world.plot(ax=axins, color='lightgray', edgecolor='black')\n",
    "geo_df.plot(ax=axins, color='red', markersize=50)\n",
    "\n",
    "# Add labels in the inset\n",
    "for x, y, label in zip(geo_df.geometry.x, geo_df.geometry.y, geo_df['station_id']):\n",
    "    axins.text(x, y, str(label), fontsize=7, ha='right', color='blue')\n",
    "\n",
    "# Set limits for the zoomed-in inset\n",
    "xmin, xmax = 44, 51  # Adjust longitude range\n",
    "ymin, ymax = 35, 40  # Adjust latitude range\n",
    "axins.set_xlim(xmin, xmax)\n",
    "axins.set_ylim(ymin, ymax)\n",
    "\n",
    "# Remove gridlines and labels in the inset for a cleaner look\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "axins.set_title(\"Zoomed-In View\", fontsize=10)\n",
    "\n",
    "# Finalize and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "Define the functions used in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spi(station_id, spi_df):\n",
    "    \"\"\"\n",
    "    Plot the Standardized Precipitation Index (SPI) for a specific station.\n",
    "    \n",
    "    Parameters:\n",
    "    station_id (int): The ID of the station to plot.\n",
    "    spi_df (DataFrame): DataFrame containing SPI values.\n",
    "    \"\"\"\n",
    "    station_data = spi_df[spi_df['station_id'] == station_id]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(station_data['data'], station_data['SPI'], marker='o', linestyle='-')\n",
    "    plt.title(f'SPI for Station {station_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plot_spi(station_id=40708, spi_df=spi_df)\n",
    "\n",
    "def plot_drought_heatmap(spi_df):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of drought conditions based on SPI values.\n",
    "    \n",
    "    Parameters:\n",
    "    spi_df (DataFrame): DataFrame containing SPI values.\n",
    "    \"\"\"\n",
    "    pivot_table = spi_df.pivot_table(index='data', columns='station_id', values='SPI')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_table, cmap='RdYlBu_r', center=0, annot=False, cbar_kws={'label': 'SPI'})\n",
    "    plt.title('Drought Conditions Heatmap')\n",
    "    plt.xlabel('Station ID')\n",
    "    plt.ylabel('Month')\n",
    "    plt.show()\n",
    "\n",
    "    plot_drought_heatmap(spi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by station and calculate SPI\n",
    "def calculate_spi(precipitation, scale=1):\n",
    "    \"\"\"\n",
    "    Calculate Standardized Precipitation Index (SPI)\n",
    "    \"\"\"\n",
    "    rolling_precip = precipitation.rolling(window=scale).mean()\n",
    "    mean = rolling_precip.mean()\n",
    "    std = rolling_precip.std()\n",
    "    z_scores = (rolling_precip - mean) / std\n",
    "    spi = norm.cdf(z_scores) * 2 - 1\n",
    "    return spi\n",
    "\n",
    "# Calculate SPI for each station\n",
    "stations = data['station_id'].unique()\n",
    "spi_results = []\n",
    "for station in stations:\n",
    "    station_data = data[data['station_id'] == station].sort_values('data')\n",
    "    station_data['SPI'] = calculate_spi(station_data['rrr24'])\n",
    "    spi_results.append(station_data)\n",
    "\n",
    "# Combine results\n",
    "spi_df = pd.concat(spi_results)\n",
    "\n",
    "# Save to a new CSV\n",
    "spi_df.to_csv('Codes\\spi_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized Precipitation Index (SPI)\n",
    "\n",
    "    rolling_precip = precipitation.rolling(window=scale).mean()\n",
    "    mean = rolling_precip.mean()\n",
    "    std = rolling_precip.std()\n",
    "    z_scores = (rolling_precip - mean) / std\n",
    "    spi = norm.cdf(z_scores) * 2 - 1\n",
    "    return spi\n",
    "\n",
    "# Calculate SPI for each station\n",
    "stations = data['station_id'].unique()\n",
    "spi_results = []\n",
    "for station in stations:\n",
    "    station_data = data[data['station_id'] == station].sort_values('data')\n",
    "    station_data['SPI'] = calculate_spi(station_data['rrr24'])\n",
    "    spi_results.append(station_data)\n",
    "\n",
    "# Combine results\n",
    "spi_df = pd.concat(spi_results)\n",
    "\n",
    "# Save to a new CSV\n",
    "spi_df.to_csv('Codes\\spi_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spi_6_month_avg(station_id, spi_df):\n",
    "    \"\"\"\n",
    "    Plot the 6-month moving average of the Standardized Precipitation Index (SPI) for a specific station.\n",
    "    \n",
    "    Parameters:\n",
    "    station_id (int): The ID of the station to plot.\n",
    "    spi_df (DataFrame): DataFrame containing SPI values with a 'date' column.\n",
    "    \"\"\"\n",
    "    # Filter data for the given station\n",
    "    station_data = spi_df[spi_df['station_id'] == station_id].copy()\n",
    "    \n",
    "    # Ensure the 'date' column is a datetime object for proper sorting\n",
    "    station_data['data'] = pd.to_datetime(station_data['data'])\n",
    "    station_data.sort_values('data', inplace=True)\n",
    "    \n",
    "    # Calculate the 6-month moving average for SPI\n",
    "    station_data['SPI_6_month_avg'] = station_data['SPI'].rolling(window=6, min_periods=1).mean()\n",
    "    \n",
    "    # Plot the 6-month moving average SPI\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(station_data['data'], station_data['SPI_6_month_avg'], marker='o', linestyle='-', label='6-Month Average')\n",
    "    plt.title(f'6-Month Average SPI for Station {station_id}')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plot_spi_6_month_avg(station_id=40708, spi_df= df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
