{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from scipy.stats import gamma, norm\n",
    "from darts.models import RNNModel,RegressionModel , RandomForest, XGBModel\n",
    "from darts.metrics import rmse, mape,mae, smape\n",
    "# from darts.utils.preprocessing import Scaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "# from darts.ad import ThresholdAD\n",
    "from darts import TimeSeries\n",
    "\n",
    "import pywt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from pptx.dml.color import RGBColor\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "\n",
    "Read and preprocess the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b43e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"ds\"])\n",
    "    df.rename(columns={\n",
    "        \"ds\": \"date\",\n",
    "        \"precip\": \"rainfall\",\n",
    "        \"SPI_1\": \"spi1\",\n",
    "        \"SPI_3\": \"spi3\",\n",
    "        \"SPI_6\": \"spi6\",\n",
    "        \"SPI_9\": \"spi9\",\n",
    "        \"SPI_12\": \"spi12\",\n",
    "        \"SPI_24\": \"spi24\",\n",
    "        \"station_id\": \"station\"\n",
    "    }, inplace=True)\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_seasonality(series):\n",
    "    decomposition = seasonal_decompose(series, model='additive', period=12, extrapolate_trend='freq')\n",
    "    deseasonalized = series - decomposition.seasonal\n",
    "    return deseasonalized, decomposition.seasonal\n",
    "\n",
    "\n",
    "def make_features(df, timescale, lags=12):\n",
    "    # Use SPI timescale and rainfall as covariates\n",
    "    data = df[[f\"spi{timescale}\", \"rainfall\"]].copy()\n",
    "    data = data.rename(columns={f\"spi{timescale}\": \"spi\"})\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # remove seasonality\n",
    "    # data['spi_deseason'] = remove_seasonality(data['spi'])\n",
    "    data['spi_deseason'], seasonal = remove_seasonality(data['spi'])\n",
    "    data['seasonal'] = seasonal\n",
    "\n",
    "    # lag features\n",
    "    for lag in range(1, lags+1):\n",
    "        data[f'spi_lag_{lag}'] = data['spi_deseason'].shift(lag)\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d6455",
   "metadata": {},
   "source": [
    "class TaylorDiagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TaylorDiagram:\n",
    "    def __init__(self, ref_std, fig=None, rect=111, label='Reference'):\n",
    "        self.ref_std = ref_std\n",
    "        self.sample_points = []\n",
    "\n",
    "        self.fig = fig if fig is not None else plt.figure(figsize=(8, 6))\n",
    "        self.ax = self.fig.add_subplot(rect, polar=True)\n",
    "\n",
    "        # Configure polar axes\n",
    "        self.ax.set_theta_zero_location('E')\n",
    "\n",
    "        self.ax.set_theta_direction(-1)\n",
    "        self.ax.set_theta_offset(np.pi / 2)\n",
    "        self.ax.set_ylim(0, 1.5 * ref_std)\n",
    "        self.ax.set_thetamin(0)\n",
    "        self.ax.set_thetamax(90)\n",
    "\n",
    "        # Set up correlation coefficient grid\n",
    "        self._setup_axes()\n",
    "\n",
    "        # Plot reference point\n",
    "        self.ax.plot([0], [ref_std], 'k*', markersize=12, label=label)\n",
    "\n",
    "    def _setup_axes(self):\n",
    "        corrs = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99, 1.0])\n",
    "        angles = np.arccos(corrs)\n",
    "\n",
    "        self.ax.set_thetagrids(np.degrees(angles), labels=[f\"{c:.2f}\" for c in corrs], fontsize=10)\n",
    "        self.ax.set_rlabel_position(135)\n",
    "        self.ax.set_ylabel('Standard Deviation', fontsize=12)\n",
    "\n",
    "        # Add radial grid lines manually\n",
    "        for angle in angles:\n",
    "            self.ax.plot([angle, angle], [0, self.ax.get_ylim()[1]], color='lightgray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    def add_sample(self, stddev, corrcoef, label, marker='o', color=None):\n",
    "        theta = np.arccos(corrcoef)\n",
    "        point, = self.ax.plot(theta, stddev, marker=marker, label=label, color=color, markersize=8)\n",
    "        self.sample_points.append(point)\n",
    "\n",
    "    def add_contours(self, levels=6, cmap='coolwarm', linewidths=1.2):\n",
    "        rs, ts = np.meshgrid(\n",
    "            np.linspace(0, self.ax.get_ylim()[1], 300),\n",
    "            np.linspace(0, np.pi / 2, 300)\n",
    "        )\n",
    "        rms = np.sqrt(\n",
    "            self.ref_std**2 + rs**2 - 2 * self.ref_std * rs * np.cos(ts)\n",
    "        )\n",
    "        contours = self.ax.contour(\n",
    "            ts, rs, rms,\n",
    "            levels=np.linspace(0, self.ax.get_ylim()[1], levels),\n",
    "            cmap=cmap,\n",
    "            linewidths=linewidths\n",
    "        )\n",
    "        self.fig.colorbar(contours, ax=self.ax, pad=0.1, orientation='vertical', label=\"RMS Difference\")\n",
    "        return contours\n",
    "\n",
    "    def show(self, title='Taylor Diagram'):\n",
    "        self.ax.set_title(title, fontsize=14, pad=20)\n",
    "        self.ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926e076",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Define Models and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bb81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WBBLSTMModel:\n",
    "    def __init__(self, wavelet='db1', level=1, **lstm_kwargs):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "        self.lstm_kwargs = lstm_kwargs\n",
    "        self.lstm_model = RNNModel(model='LSTM', **lstm_kwargs)\n",
    "        \n",
    "    def wavelet_decompose(self, series: TimeSeries):\n",
    "        coeffs = pywt.wavedec(series.values().flatten(), self.wavelet, level=self.level)\n",
    "        return coeffs\n",
    "    \n",
    "    def wavelet_reconstruct(self, coeffs):\n",
    "        return pywt.waverec(coeffs, self.wavelet)\n",
    "    \n",
    "    def _make_multivariate(self, series):\n",
    "        coeffs = self.wavelet_decompose(series)\n",
    "        min_len = min(len(c) for c in coeffs)\n",
    "        stacked = np.column_stack([c[:min_len] for c in coeffs])\n",
    "        ts = TimeSeries.from_times_and_values(\n",
    "            pd.date_range(series.start_time(), periods=min_len, freq=series.freq),\n",
    "            stacked.astype(np.float32)\n",
    "        )\n",
    "        ts = series.time_index[:min_len]\n",
    "        # build a DataFrame: each coeff is a column\n",
    "        col_names = [f\"c{i}\" for i in range(len(coeffs))]\n",
    "        df = pd.DataFrame(stacked, index=ts, columns=col_names)\n",
    "\n",
    "        # add lagged versions as extra columns\n",
    "        for lag in [1,2,3,6,12]:\n",
    "            for name in col_names:\n",
    "                df[f\"{name}_lag{lag}\"] = df[name].shift(lag)\n",
    "\n",
    "        # drop NA rows created by shifting\n",
    "        df = df.dropna()\n",
    "\n",
    "        # convert to TimeSeries\n",
    "        return TimeSeries.from_dataframe(df.astype(np.float32))\n",
    "\n",
    "\n",
    "    def fit(self, series, future_covariates=None):\n",
    "        mv = self._make_multivariate(series)\n",
    "        cov = future_covariates.slice_intersect(series) if future_covariates else None\n",
    "        self.lstm_model.fit(mv, future_covariates=cov)\n",
    "\n",
    "    def predict(self, n, series, future_covariates=None):\n",
    "        orig_coeffs = self.wavelet_decompose(series)  \n",
    "        num_coefs   = len(orig_coeffs)\n",
    "        mv = self._make_multivariate(series)\n",
    "        cov = future_covariates.slice_intersect(series) if future_covariates else None\n",
    "        pred_mv = self.lstm_model.predict(n, series=mv, future_covariates=cov)\n",
    "        pred_coeffs = [pred_mv.values()[:,i] for i in range(num_coefs)]\n",
    "\n",
    "        \n",
    "        ext_coeffs = [\n",
    "        np.concatenate([orig, pred])\n",
    "        for orig, pred in zip(orig_coeffs, pred_coeffs)\n",
    "    ]\n",
    "\n",
    "\n",
    "        rec = self.wavelet_reconstruct(ext_coeffs)\n",
    "        rec = rec[-n:]\n",
    "        idx = pd.date_range(series.end_time()+series.freq, periods=n, freq=series.freq)\n",
    "        return TimeSeries.from_times_and_values(idx, rec.reshape(-1,1))\n",
    "      \n",
    "    \n",
    "    \n",
    "    def save(self, path):\n",
    "        # os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Save LSTM model\n",
    "        self.lstm_model.save(path)\n",
    "\n",
    "        # Save wavelet settings and kwargs\n",
    "        meta = {\n",
    "        'wavelet': self.wavelet,\n",
    "            'level': self.level,\n",
    "            'lstm_kwargs': self.lstm_kwargs\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(os.path.dirname(path), \"WBILSTMmeta.json\"), \"w\") as f:\n",
    "            json.dump(meta, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(os.path.join(os.path.dirname(path), \"WBILSTMmeta.json\"), \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "\n",
    "        wavelet = meta['wavelet']\n",
    "        level = meta['level']\n",
    "        lstm_kwargs = meta['lstm_kwargs']\n",
    "        instance = cls(wavelet=wavelet, level=level, **lstm_kwargs)\n",
    "\n",
    "        # instance = cls(**meta)\n",
    "        instance.lstm_model = RNNModel.load(path)\n",
    "        return instance\n",
    "\n",
    "\n",
    "\n",
    "window_size = 12\n",
    "num_epochs=100\n",
    "horizon = 1 \n",
    "\n",
    "\n",
    "model_constructors = {\n",
    "       'ExtraTF': lambda: XGBModel(\n",
    "         lags=window_size,\n",
    "        lags_future_covariates=[0],\n",
    "         output_chunk_length=horizon,\n",
    "         random_state=SEED,\n",
    "         objective='reg:squarederror'\n",
    "    ),\n",
    "    'RandomRF': lambda: RandomForest(\n",
    "         lags=window_size,\n",
    "         lags_future_covariates=[0],\n",
    "         output_chunk_length=horizon,\n",
    "         n_estimators=200,\n",
    "         criterion=\"absolute_error\",\n",
    "         random_state=SEED\n",
    "    ),\n",
    "    'SVR': lambda: RegressionModel(\n",
    "         model=SVR(kernel='rbf'),\n",
    "         lags_future_covariates=[0],\n",
    "         lags=window_size,\n",
    "         output_chunk_length=horizon\n",
    "    ),\n",
    "    'LSTM': lambda: RNNModel(\n",
    "         model='LSTM',\n",
    "         input_chunk_length=window_size,\n",
    "         output_chunk_length=horizon,\n",
    "         hidden_dim=25,\n",
    "         n_rnn_layers=2,\n",
    "         dropout=0.1,\n",
    "         batch_size=16,\n",
    "         n_epochs=num_epochs,\n",
    "         optimizer_kwargs={'lr':1e-3},\n",
    "         random_state=SEED\n",
    "    ),\n",
    "    'WBBLSTM'   : lambda: WBBLSTMModel(\n",
    "        wavelet='db1',\n",
    "        level=1,\n",
    "        input_chunk_length=window_size,\n",
    "        output_chunk_length=horizon,\n",
    "        hidden_dim=25,\n",
    "        n_rnn_layers=2,\n",
    "        dropout=0.1,\n",
    "        batch_size=16,\n",
    "        n_epochs=num_epochs,\n",
    "        optimizer_kwargs={'lr':1e-3},\n",
    "        random_state=SEED\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4fc8a",
   "metadata": {},
   "source": [
    "main loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16601ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Station 40700 | 3 ===\n",
      "Model ExtraTF already trained. Skipping.\n",
      "!!!@@@-----Loading ExtraTF model from results/40700/3/ExtraTF‚Ä¶\n",
      "Model RandomRF already trained. Skipping.\n",
      "!!!@@@-----Loading RandomRF model from results/40700/3/RandomRF‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SVR already trained. Skipping.\n",
      "!!!@@@-----Loading SVR model from results/40700/3/SVR‚Ä¶\n",
      "Model LSTM already trained. Skipping.\n",
      "!!!@@@-----Loading LSTM model from results/40700/3/LSTM‚Ä¶\n",
      "Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 27.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 10.4 K | train\n",
      "6 | V               | Linear           | 312    | train\n",
      "-------------------------------------------------------------\n",
      "10.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.7 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 120.69it/s, train_loss=0.137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 119.84it/s, train_loss=0.137]\n",
      " saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 27.24it/s]\n",
      "std_p: 0.55, corr: 0.55, rmse: 0.71, mae_v: 0.51,sm: 116.86 - ExtraTF \n",
      "std_p: 0.51, corr: 0.72, rmse: 0.61, mae_v: 0.38,sm: 99.15 - RandomRF \n",
      "std_p: 0.36, corr: 0.68, rmse: 0.68, mae_v: 0.42,sm: 108.87 - SVR \n",
      "std_p: 0.48, corr: 0.76, rmse: 0.65, mae_v: 0.43,sm: 106.23 - LSTM \n",
      "std_p: 0.24, corr: 0.17, rmse: 0.86, mae_v: 0.62,sm: 153.56 - WBBLSTM \n",
      "‚úîÔ∏è Done with 40700 | 3\n",
      "\n",
      "üìÑ Results saved as CSV: results/results_summary.csv\n"
     ]
    }
   ],
   "source": [
    "results = {}  \n",
    "base_dir = \"results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "df = load_data('./finaldata.csv')\n",
    "stations = df['station'].unique()\n",
    "summary = []\n",
    "\n",
    "def model_score(stats):\n",
    "    std_o, std_p, corr, rmse, mape_val, _ = stats\n",
    "    std_diff = abs(std_o - std_p)\n",
    "    return (\n",
    "        rmse + mape_val + std_diff - corr  \n",
    "    )\n",
    "\n",
    "plot_start = pd.Timestamp(\"2017-01-01\")\n",
    "plot_end   = pd.Timestamp(\"2024-04-01\")\n",
    "\n",
    "# for st in stations:\n",
    "for st in [40700]:\n",
    "    df_st = df[df['station'] == st].copy()\n",
    "    # for ts in [1,3,6,9,12,24]:\n",
    "    for ts in [3]:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        results[st] = {}\n",
    "        data = make_features(df_st, ts)\n",
    "        print(f\"\\n=== Station {st} | {ts} ===\")\n",
    "        target_series = TimeSeries.from_series(data[\"spi_deseason\"])\n",
    "        target_series = target_series.astype(np.float32)  \n",
    "\n",
    "        covariates = TimeSeries.from_dataframe(\n",
    "            data[[\"rainfall\"] + [f\"spi_lag_{i}\" for i in range(1, 13)]]\n",
    "        )\n",
    "        covariates = covariates.astype(np.float32)\n",
    "        target_series, covariates = target_series.slice_intersect(covariates), covariates.slice_intersect(target_series)\n",
    "\n",
    "        train, val = target_series.split_before(0.8)\n",
    "        target_series.slice(plot_start, plot_end).plot(label=\"Observed\", lw=2)\n",
    "\n",
    "        model_folder = os.path.join(base_dir, f\"{st}/{ts}\")\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "        model_stats = {}\n",
    "        forecasts = {}\n",
    "        for name, mk in model_constructors.items():\n",
    "            model_path = os.path.join(model_folder, name)\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"Model {name} already trained. Skipping.\")\n",
    "                model_class = {\n",
    "                    'LSTM': RNNModel,\n",
    "                    'SVR': RegressionModel,\n",
    "                    'RandomRF': RandomForest,\n",
    "                    'ExtraTF': XGBModel,\n",
    "                    'WBBLSTM': WBBLSTMModel\n",
    "                }[name]\n",
    "                print(f\"!!!@@@-----Loading {name} model from {model_path}‚Ä¶\")\n",
    "                model = model_class.load(model_path)\n",
    "\n",
    "            else:\n",
    "                print(f\"!!!@@@------Training {name}‚Ä¶\", end='')\n",
    "                model = mk()\n",
    "                model.fit(series=train, future_covariates=covariates)\n",
    "                model.save(model_path)\n",
    "\n",
    "                print(\" saved.\")\n",
    "\n",
    "            forecast = model.predict(len(val), series=train, future_covariates=covariates)\n",
    "            forecast_zoom = forecast.slice(plot_start, plot_end)\n",
    "            forecast_zoom.plot(label=name, lw=1.8)\n",
    "            o = val.values().flatten()\n",
    "            p = forecast.values().flatten()\n",
    "\n",
    "            corr = pearsonr(o, p)[0]\n",
    "            if corr < 0:\n",
    "                print('negative corr')\n",
    "                p = -p\n",
    "                corr = -corr\n",
    "\n",
    "\n",
    "            rm   = np.sqrt(mean_squared_error(o, p))\n",
    "            mae_v= mean_absolute_error(o, p)\n",
    "            sm   = np.mean(2 * np.abs(o-p) / (np.abs(o)+np.abs(p))) * 100\n",
    "\n",
    "            model_stats[name] = (np.std(o), np.std(p), corr, rm, mae_v, sm)\n",
    "            \n",
    "            forecasts[name] = forecast\n",
    "\n",
    "\n",
    "        plt.title(f\"Model Comparison ‚Äî Station {st} | Timescale: SPI-{ts}\", fontsize=14)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"SPI Value\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(model_folder, f\"all_models_{st}_{ts}.png\"), dpi=300)\n",
    "        plt.close()\n",
    "        results[st][f\"spi_{ts}\"] = model_stats\n",
    "        for model_name, stats in model_stats.items():\n",
    "                        std_o, std_p, corr, rmse, mae, smape = stats\n",
    "                        score = model_score(stats)\n",
    "                        summary.append({\n",
    "                            'Station': st,\n",
    "                            'Timescale': ts,\n",
    "                            'Model': model_name,\n",
    "                            'STD_Obs': std_o,\n",
    "                            'STD_Pred': std_p,\n",
    "                            'Corr': corr,\n",
    "                            'RMSE': rmse,\n",
    "                            'MAE': mae,\n",
    "                            'SMAPE': smape,\n",
    "                            'Score': score,\n",
    "                        })\n",
    "\n",
    "        # Taylor Diagram\n",
    "        ref_std = list(model_stats.values())[0][0]\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        taylor = TaylorDiagram(ref_std, fig, label='Observed')\n",
    "        for name, (std_o, std_p, corr, rmse,mae_v,sm) in model_stats.items():\n",
    "            print(f\"std_p: {std_p:.2f}, corr: {corr:.2f}, rmse: {rmse:.2f}, mae_v: {mae_v:.2f},sm: {sm:.2f} - {name} \")\n",
    "            if np.isnan(std_p) or np.isnan(corr) or std_p == 0:\n",
    "                print(f\"‚ö†Ô∏è Skipping {name} due to invalid metrics.\")\n",
    "                continue\n",
    "            taylor.add_sample(std_p, corr, label=name)\n",
    "        taylor.add_contours()\n",
    "        plt.legend()\n",
    "        plt.title(f\"Taylor Diagram: Station {st} | {ts}\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(os.path.join(model_folder, f\"taylor_{st}_{ts}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Best model\n",
    "        best_model = min(model_stats.items(), key=lambda x: model_score(x[1]))[0]\n",
    "\n",
    "        # Plot val vs pred\n",
    "        _, val = target_series.split_before(0.8)\n",
    "        time_idx = val.time_index\n",
    "        pred = forecasts[best_model]\n",
    "        val_df  = val.to_dataframe()  \n",
    "        pred_df = pred.to_dataframe()\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        val.plot(label=\"Observed\")\n",
    "        pred.plot(label=f\"Forecast - {best_model}\")\n",
    "        plt.title(f\"Validation vs Prediction ‚Äî {st} | {ts}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(model_folder, f\"val_vs_pred_{st}_{ts}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"‚úîÔ∏è Done with {st} | {ts}\\n\")\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    # Identify best model (custom score and RMSE)\n",
    "    df_summary['Best_Model'] = df_summary.groupby(['Station', 'Timescale'])['Score'].transform(\n",
    "        lambda x: x == x.min()\n",
    "    )\n",
    "    df_summary['Best_RMSE'] = df_summary.groupby(['Station', 'Timescale'])['RMSE'].transform(\n",
    "        lambda x: x == x.min()\n",
    "    )\n",
    "    df_summary['Winner'] = df_summary.apply(\n",
    "    lambda row: '‚úîÔ∏è' if row['Best_Model'] else '', axis=1\n",
    ")\n",
    "\n",
    "    # Save CSV\n",
    "    df_summary_path = os.path.join(base_dir, 'results_summary.csv')\n",
    "    df_summary.to_csv(df_summary_path, index=False)\n",
    "    print(f\"üìÑ Results saved as CSV: {df_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489fa27",
   "metadata": {},
   "source": [
    "forecasting with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # spi_df = future.pd_dataframe().reset_index()\n",
    "        # spi_df['year'] = pd.to_datetime(spi_df['date']).dt.year\n",
    "        # spi_df['month'] = pd.to_datetime(spi_df['date']).dt.month\n",
    "\n",
    "        # # SPI heatmap\n",
    "        # heatmap_data = spi_df.pivot_table(index='year', columns='month', values=\"spi_deseason\")\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # sns.heatmap(heatmap_data, cmap='rocket', center=0, annot=True, fmt=\".2f\")\n",
    "        # plt.title(f\"SPI Heatmap ‚Äî {ts} ‚Äî {st}\")\n",
    "        # plt.xlabel(\"Month\")\n",
    "        # plt.ylabel(\"Year\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.grid(False)\n",
    "        # plt.savefig(os.path.join(base_dir, f\"{st}/heatmap_{ts}.png\"))\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # spi_df['category'] = pd.cut(spi_df[\"spi_deseason\"], bins=[-np.inf, -1, 1, np.inf], labels=['Dry', 'Normal', 'Wet'])\n",
    "        # colors = {'Dry': 'red', 'Normal': 'gray', 'Wet': 'blue'}\n",
    "\n",
    "        # plt.figure(figsize=(14, 6))\n",
    "        # for category, color in colors.items():\n",
    "        #     mask = spi_df['category'] == category\n",
    "        #     plt.scatter(spi_df['date'][mask], spi_df['spi_deseason'][mask], c=color, label=category)\n",
    "\n",
    "        # plt.axhline(0, color='black', lw=1, linestyle='--')\n",
    "        # plt.title(f\"SPI Categories: Dry / Normal / Wet ‚Äî {st} | {ts}\")\n",
    "        # plt.xlabel(\"Date\")\n",
    "        # plt.ylabel(\"SPI Value\")\n",
    "        # plt.grid(True)\n",
    "        # plt.tight_layout()\n",
    "        # plt.legend()\n",
    "        # plt.savefig(os.path.join(base_dir, f\"{st}/scatter_{ts}.png\"))\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # # Use a simple threshold detector\n",
    "        # detector = ThresholdAD(low_threshold=-1.5, high_threshold=1.5)\n",
    "        # anomalies = detector.detect(full_series)\n",
    "\n",
    "        # # Plot\n",
    "        # full_series.plot(label=\"SPI\")\n",
    "        # anomalies.plot(label=\"Anomalies\", color='red', marker='o')\n",
    "        # plt.legend()\n",
    "        # plt.title(f\"Darts Anomaly Detection ‚Äî {station_id} | {col}\")\n",
    "        # plt.show()\n",
    "\n",
    "#     # best.fit(series=target_series, future_covariates=covariates)\n",
    "        #     best.fit(series=target_series)\n",
    "\n",
    "        \n",
    "\n",
    "        # cov_df = covariates.pd_dataframe()  # or however you get a pandas DataFrame\n",
    "        # cov_df['month'] = cov_df.index.month\n",
    "\n",
    "        # # Compute monthly means for each column except 'month'\n",
    "        # monthly_means = cov_df.groupby('month').mean()\n",
    "\n",
    "        # # 2) Build the future date index from May‚ÄØ2024 to April‚ÄØ2051\n",
    "        # last_hist = covariates.end_time()         # e.g. Timestamp('2024-04-01 00:00:00')\n",
    "        # start_future = last_hist + MonthEnd(1)    # gives end of month, but for MS freq it lines up\n",
    "        # future_idx = pd.date_range(start=start_future,\n",
    "        #                         end=\"2051-04-01\",\n",
    "        #                         freq=\"MS\")\n",
    "\n",
    "        # # 3) Create a DataFrame for future covariates by mapping each future month to its climatology\n",
    "        # df_future = pd.DataFrame(index=future_idx)\n",
    "\n",
    "        # # For each covariate column, fill with the corresponding monthly mean\n",
    "        # for col in monthly_means.columns:\n",
    "        #     df_future[col] = [monthly_means.loc[m, col] for m in df_future.index.month]\n",
    "\n",
    "        # # 4) Convert to a Darts TimeSeries\n",
    "        # full_df = pd.concat([covariates.pd_dataframe(), df_future])\n",
    "        # future_covariates = TimeSeries.from_dataframe(full_df)\n",
    "        # future_covariates = TimeSeries.from_dataframe(df_future)\n",
    "        # Forecast to 2050\n",
    "        # horizon = (pd.Timestamp(\"2050-12-01\") - target_series.end_time()).days // 30\n",
    "        # future = best.predict(horizon, series=target_series,future_covariates=future_covariates)\n",
    "        # future = best.predict(horizon, series=target_series)\n",
    "\n",
    "\n",
    "        # if not isinstance(future.time_index, pd.DatetimeIndex):\n",
    "        #     future = TimeSeries.from_times_and_values(\n",
    "        #         pd.date_range(\n",
    "        #             start=target_series.end_time() + pd.DateOffset(months=1),\n",
    "        #             periods=len(future),\n",
    "        #             freq=\"MS\"\n",
    "        #         ),\n",
    "        #         future.values(),\n",
    "        #         columns=target_series.components\n",
    "        #     )\n",
    "\n",
    "        # historical = target_series\n",
    "        # full_series = historical.append(future)\n",
    "\n",
    "        # plt.figure(figsize=(12, 4))\n",
    "        # full_series.plot(label=\"SPI\")\n",
    "        # plt.axvline(x=historical.end_time(), color='r', linestyle='--', label=\"Forecast Start\")\n",
    "        # plt.title(f\"Forecast 2050 {st} | {ts}: {best_model}\")\n",
    "\n",
    "        # plt.xlabel(\"Time\")\n",
    "        # plt.ylabel(\"SPI\")\n",
    "        # plt.grid(True)\n",
    "        # plt.legend()\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig(os.path.join(model_folder, f\"{st}/Forecast_{ts}.png\"))\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a3e1",
   "metadata": {},
   "source": [
    "Loop over stations & timescales, \n",
    " forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf9ee0",
   "metadata": {},
   "source": [
    "then auto slide creation or pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_path = os.path.join(base_dir, \"SPI_Results_Summary.pptx\")\n",
    "prs = Presentation()\n",
    "\n",
    "title_slide_layout = prs.slide_layouts[0]\n",
    "blank_slide_layout = prs.slide_layouts[6]\n",
    "\n",
    "# Title slide\n",
    "slide = prs.slides.add_slide(title_slide_layout)\n",
    "slide.shapes.title.text = \"SPI Forecast & Evaluation Summary\"\n",
    "# slide.placeholders[1].text = \"Auto-generated using python-pptx\\nIncludes Taylor Diagrams, Heatmaps, Model Metrics & Forecasts\"\n",
    "\n",
    "# image_summaries = {\n",
    "#     \"val_vs_pred\": \"Comparison of predicted vs. actual SPI values. Good alignment indicates accurate forecasting.\",\n",
    "#     \"heatmap\": \"Heatmap of forecast performance over time. Brighter regions indicate higher error or uncertainty.\",\n",
    "#     \"taylor\": \"Taylor diagram summarizing model skill. Closer proximity to reference indicates better performance.\",\n",
    "#     \"scatter\": \"Scatter plot of predicted vs. observed SPI. Closer points to diagonal line show better predictions.\"\n",
    "# }\n",
    "\n",
    "def generate_summary(img_type, model_metrics):\n",
    "    \"\"\"Create a smart summary based on image type and model metrics.\"\"\"\n",
    "    if not model_metrics:\n",
    "        return \"Performance data not available.\"\n",
    "\n",
    "    # Pick the best model based on RMSE (you can use another metric too)\n",
    "    best_model, (std_o, std_p, corr, rmse, mape,sm) = min(model_metrics.items(), key=lambda x: x[1][3])  # sort by RMSE\n",
    "\n",
    "    # Interpret performance\n",
    "    if rmse < 0.3 and mape < 10 and corr > 0.85:\n",
    "        perf = \"excellent\"\n",
    "    elif rmse < 0.6 and mape < 20 and corr > 0.65:\n",
    "        perf = \"reasonable\"\n",
    "    else:\n",
    "        perf = \"poor\"\n",
    "\n",
    "    # Now create summaries\n",
    "    if img_type == \"val_vs_pred\":\n",
    "        return f\"Predicted vs. observed SPI using {best_model}. Alignment is {perf}, with RMSE={rmse:.2f}, Corr={corr:.2f}.\"\n",
    "    elif img_type == \"heatmap\":\n",
    "        return f\"Heatmap of error over time for {best_model}. Performance is {perf}, with average MAPE={mape:.1f}%.\"\n",
    "    elif img_type == \"taylor\":\n",
    "        return f\"Taylor diagram showing model spread vs. observed. {best_model} shows {perf} alignment with reference point.\"\n",
    "    elif img_type == \"scatter\":\n",
    "        return f\"Scatter plot for {best_model}. {perf.capitalize()} correlation between predictions and observations (Corr={corr:.2f}).\"\n",
    "    else:\n",
    "        return \"Performance visualization.\"\n",
    "\n",
    "# Loop through all stations\n",
    "for station_id in sorted(os.listdir(base_dir)):\n",
    "    station_path = os.path.join(base_dir, station_id)\n",
    "    if not os.path.isdir(station_path):\n",
    "        continue\n",
    "\n",
    "    for col in results.get(int(station_id), {}):  # Ensure `station_id` matches results keys\n",
    "        # Add slide for this station/SPI\n",
    "        slide = prs.slides.add_slide(blank_slide_layout)\n",
    "        # Add title\n",
    "        title_box = slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(0.5))\n",
    "        tf = title_box.text_frame\n",
    "        tf.text = f\"Station {station_id} ‚Äî {col}\"\n",
    "        tf.paragraphs[0].font.size = Pt(24)\n",
    "        tf.paragraphs[0].font.bold = True\n",
    "\n",
    "        # Add metrics table\n",
    "        metrics = results[int(station_id)][col]\n",
    "        rows, cols = len(metrics) + 1, 6\n",
    "        table = slide.shapes.add_table(rows, cols, Inches(0.5), Inches(1), Inches(8), Inches(0.6 + rows * 0.4)).table\n",
    "        table.cell(0, 0).text = \"Model\"\n",
    "        table.cell(0, 1).text = \"RMSE\"\n",
    "        table.cell(0, 2).text = \"MAPE\"\n",
    "        table.cell(0, 3).text = \"Corr\"\n",
    "        table.cell(0, 4).text = \"Std. Dev (Pred)\"\n",
    "        table.cell(0, 5).text = \"smape\"\n",
    "\n",
    "        for i, (model_name, (std_o, std_p, corr, rmse, mape,sm)) in enumerate(metrics.items(), start=1):\n",
    "            table.cell(i, 0).text = model_name\n",
    "            table.cell(i, 1).text = f\"{rmse:.3f}\"\n",
    "            table.cell(i, 2).text = f\"{mape:.2f}%\"\n",
    "            table.cell(i, 3).text = f\"{corr:.2f}\"\n",
    "            table.cell(i, 4).text = f\"{std_p:.2f}\"\n",
    "            table.cell(i, 5).text = f\"{sm:.2f}\"\n",
    "\n",
    "        # Add first two images\n",
    "        # image_files = [\"taylor\",\"val_vs_pred\",  \"heatmap\", \"scatter\"]\n",
    "        image_files = [\"taylor\",\"val_vs_pred\",\"Forecast\", \"heatmap\",  \"scatter\"]\n",
    "        img_slide_count = 0\n",
    "        img_group = []\n",
    "\n",
    "        for img_type in image_files:\n",
    "            img_path = os.path.join(station_path, f\"{img_type}_{col}.png\")\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "\n",
    "            # Create a new slide for each image\n",
    "            current_slide = prs.slides.add_slide(blank_slide_layout)\n",
    "\n",
    "            # Add slide title\n",
    "            sub_title_box = current_slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(0.5))\n",
    "            sub_tf = sub_title_box.text_frame\n",
    "            sub_tf.text = f\"Station {station_id} ‚Äî {col} ({img_type.replace('_', ' ').title()})\"\n",
    "            sub_tf.paragraphs[0].font.size = Pt(20)\n",
    "            sub_tf.paragraphs[0].font.bold = True\n",
    "\n",
    "            # Add image\n",
    "            x = Inches(0.5)\n",
    "            y_img = Inches(1.0)\n",
    "            current_slide.shapes.add_picture(img_path, x, y_img, width=Inches(8.5))  # Full width if needed\n",
    "\n",
    "            # Add summary text\n",
    "            y_text = y_img + Inches(5.8)\n",
    "            model_metrics = results[int(station_id)][col]\n",
    "            summary_text = generate_summary(img_type, model_metrics)\n",
    "\n",
    "            textbox = current_slide.shapes.add_textbox(x, y_text, width=Inches(8.5), height=Inches(1))\n",
    "            tf = textbox.text_frame\n",
    "            tf.text = summary_text\n",
    "            tf.paragraphs[0].font.size = Pt(12)\n",
    "\n",
    "        # for img_type in image_files:\n",
    "        #     img_path = os.path.join(station_path, f\"{img_type}_{col}.png\")\n",
    "        #     if os.path.exists(img_path):\n",
    "        #         img_group.append(img_path)\n",
    "\n",
    "        # # Split image group into chunks of 2\n",
    "        # for i in range(0, len(img_group), 2):\n",
    "        #     if i == 0:\n",
    "        #         # Use the first slide already created\n",
    "        #         current_slide = slide\n",
    "        #     else:\n",
    "        #         current_slide = prs.slides.add_slide(blank_slide_layout)\n",
    "\n",
    "        #         # Add title for extra image slides\n",
    "        #         sub_title_box = current_slide.shapes.add_textbox(Inches(0.5), Inches(0.3), Inches(9), Inches(0.5))\n",
    "        #         sub_tf = sub_title_box.text_frame\n",
    "        #         sub_tf.text = f\"Station {station_id} ‚Äî {col} (Images {i + 1}‚Äì{min(i+2, len(img_group))})\"\n",
    "        #         sub_tf.paragraphs[0].font.size = Pt(20)\n",
    "        #         sub_tf.paragraphs[0].font.bold = True\n",
    "\n",
    "        #     for j, img_path in enumerate(img_group[i:i+2]):\n",
    "        #         x = Inches(0.5 + j * 5)  # Side by side\n",
    "        #         y_img  = Inches(2.7 if i == 0 else 1.0)\n",
    "        #         y_text = y_img + Inches(3.6)  # Position text below image\n",
    "        #         current_slide.shapes.add_picture(img_path, x, y_img, width=Inches(4.5))\n",
    "\n",
    "        #         # Extract image type to get its description\n",
    "        #         img_filename = os.path.basename(img_path)\n",
    "        #         img_type = img_filename.split(\"_\")[0]\n",
    "\n",
    "        #         model_metrics = results[int(station_id)][col]\n",
    "        #         summary_text = generate_summary(img_type, model_metrics)\n",
    "\n",
    "\n",
    "        #         textbox = current_slide.shapes.add_textbox(x, y_text, width=Inches(4.5), height=Inches(1))\n",
    "        #         tf = textbox.text_frame\n",
    "        #         tf.text = summary_text\n",
    "        #         tf.paragraphs[0].font.size = Pt(12)\n",
    "\n",
    "# Save presentation\n",
    "prs.save(ppt_path)\n",
    "print(f\"‚úÖ Presentation saved to: {ppt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680435b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "base_dir = \"all_results\"\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading(\"SPI Forecast & Evaluation Summary\", 0)\n",
    "spi_periods = [\"1\", \"3\", \"6\", \"9\", \"12\", \"24\"]\n",
    "\n",
    "\n",
    "def generate_summary(img_type, model_metrics):\n",
    "    if not model_metrics:\n",
    "        return None\n",
    "\n",
    "    best_model, (std_o, std_p, corr, rmse, mape, sm) = min(model_metrics.items(), key=lambda x: x[1][3])\n",
    "    if rmse < 0.3 and mape < 10 and corr > 0.85:\n",
    "        perf = \"excellent\"\n",
    "    elif rmse < 0.6 and mape < 20 and corr > 0.65:\n",
    "        perf = \"reasonable\"\n",
    "    else:\n",
    "        perf = \"poor\"\n",
    "\n",
    "    if img_type == \"val_vs_pred\":\n",
    "        return f\"{best_model}: RMSE={rmse:.2f}, Corr={corr:.2f} ‚Äî {perf} performance.\"\n",
    "    elif img_type == \"heatmap\":\n",
    "        return f\"{best_model} temporal map: Avg. MAPE={mape:.1f}% ‚Äî {perf} performance.\"\n",
    "    elif img_type == \"taylor\":\n",
    "        return f\"{best_model} Taylor diagram ‚Äî {perf} skill alignment.\"\n",
    "    return None\n",
    "\n",
    "# Inputs\n",
    "image_types  = {\n",
    "    \"taylor\": \"Taylor Diagram\",\n",
    "    \"heatmap\": \"Temporal Map\",\n",
    "    \"val_vs_pred\": \"Validation vs. Prediction\"\n",
    "}\n",
    "\n",
    "# def add_image(doc, img_path, caption, width=Inches(2.9)):\n",
    "#     if not os.path.exists(img_path):\n",
    "#         return\n",
    "#     doc.add_picture(img_path, width=width)\n",
    "#     last_paragraph = doc.paragraphs[-1]\n",
    "#     last_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "#     caption_paragraph = doc.add_paragraph(caption)\n",
    "#     caption_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "#     caption_paragraph.style.font.size = Pt(9)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def crop_bottom_whitespace(image_path, save_path=None, crop_ratio=0.3):\n",
    "    \"\"\"Crop the bottom portion of the image.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        cropped_img = img.crop((0, 0, width, int(height * (1 - crop_ratio))))\n",
    "        cropped_img.save(save_path or image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error cropping {image_path}: {e}\")\n",
    "\n",
    "# Loop through station folders\n",
    "for station_id in sorted(os.listdir(base_dir)):\n",
    "    station_path = os.path.join(base_dir, station_id)\n",
    "    if not os.path.isdir(station_path) or not station_id.isdigit():\n",
    "        continue\n",
    "\n",
    "    doc.add_heading(f\"Station {station_id}\", level=1)\n",
    "\n",
    "    for spi in spi_periods:\n",
    "        doc.add_heading(f\"SPI-{spi}\", level=2)\n",
    "\n",
    "        table = doc.add_table(rows=1, cols=3)\n",
    "        table.autofit = True\n",
    "        row = table.rows[0]\n",
    "\n",
    "        for i, prefix in enumerate([\"taylor\", \"heatmap\", \"val_vs_pred\"]):\n",
    "            file_name = f\"{prefix}_SPI_{spi}.png\"\n",
    "            img_path = os.path.join(station_path, file_name)\n",
    "\n",
    "            # Crop white space from Taylor diagram if needed\n",
    "            if prefix == \"taylor\" and os.path.exists(img_path):\n",
    "                crop_bottom_whitespace(img_path)\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                paragraph = row.cells[i].paragraphs[0]\n",
    "                run = paragraph.add_run()\n",
    "                run.add_picture(img_path, width=Inches(2.2))\n",
    "                row.cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "        doc.add_paragraph()  # slight space between rows\n",
    "\n",
    "    doc.add_page_break()\n",
    "\n",
    "doc_path = os.path.join(base_dir, \"SPI_Results_Summary.docx\")\n",
    "doc.save(doc_path)\n",
    "print(f\"‚úÖ Word document saved to: {doc_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb6bbd",
   "metadata": {},
   "source": [
    "Iran Stations Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8473505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch, Rectangle\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from scipy.interpolate import Rbf\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# === Load shapefiles ===\n",
    "iran = gpd.read_file('materials/iran.shp')   \n",
    "sea = gpd.read_file('materials/seas.shp') \n",
    "\n",
    "# === Load and prepare station data ===\n",
    "stations_df = pd.read_csv('../main_data.csv')\n",
    "stations = stations_df[['station_id', 'station_name', 'station_elevation', 'lat', 'lon']].drop_duplicates()\n",
    "geometry = [Point(xy) for xy in zip(stations['lon'], stations['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(stations, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "buffer = 0.3  # Optional: adds margin around stations\n",
    "xmin, xmax = geo_df.geometry.x.min() - buffer, geo_df.geometry.x.max() + buffer\n",
    "ymin, ymax = geo_df.geometry.y.min() - buffer, geo_df.geometry.y.max() + buffer\n",
    "\n",
    "\n",
    "# === Interpolate elevation ===\n",
    "x = geo_df.geometry.x.values\n",
    "y = geo_df.geometry.y.values\n",
    "z = geo_df['station_elevation'].values\n",
    "grid_res = 500\n",
    "# xi = np.linspace(iran.total_bounds[0], iran.total_bounds[2], grid_res)\n",
    "# yi = np.linspace(iran.total_bounds[1], iran.total_bounds[3], grid_res)\n",
    "xi = np.linspace(xmin, xmax, grid_res)\n",
    "yi = np.linspace(ymin, ymax, grid_res)\n",
    "xi, yi = np.meshgrid(xi, yi)\n",
    "rbf = Rbf(x, y, z, function='linear')\n",
    "zi = rbf(xi, yi)\n",
    "\n",
    "# Create a mask from the Iran polygon\n",
    "iran_shape = iran.unary_union  # In case it's a multi-polygon\n",
    "mask = np.ones_like(zi, dtype=bool)\n",
    "\n",
    "# Build a Path object for masking\n",
    "# iran_path = Path(iran_shape.exterior.coords[:])\n",
    "\n",
    "# Go through each grid point and mask if it's outside\n",
    "for i in range(zi.shape[0]):\n",
    "    for j in range(zi.shape[1]):\n",
    "        point = (xi[0, j], yi[i, 0])  # Get X, Y of this pixel\n",
    "        if iran_shape.contains(Point(point)):\n",
    "            mask[i, j] = False\n",
    "\n",
    "# Apply mask to elevation\n",
    "zi_masked = np.ma.masked_array(zi, mask=mask)\n",
    "\n",
    "# === Define zoom region ===\n",
    "# # Set the zoom box manually for a region (e.g., Ardabil or any other)\n",
    "# zoom_xmin, zoom_xmax = 46.5, 49.5\n",
    "# zoom_ymin, zoom_ymax = 37.0, 39.0\n",
    "\n",
    "# === Setup figure ===\n",
    "fig = plt.figure(figsize=(5, 8))\n",
    "\n",
    "# --- Left: Full map of Iran ---\n",
    "ax_iran = fig.add_axes([0.02, 0.65, 0.5, 0.5])  # Top-left corner\n",
    "sea.plot(ax=ax_iran, color='lightblue')\n",
    "iran.plot(ax=ax_iran, color='lightgray', edgecolor='black')\n",
    "ax_iran.set_xticks([]); ax_iran.set_yticks([]); ax_iran.set_title('Iran Overview', fontsize=12)\n",
    "\n",
    "# # Draw zoom rectangle on Iran map\n",
    "# rect = Rectangle((zoom_xmin, zoom_ymin), zoom_xmax - zoom_xmin, zoom_ymax - zoom_ymin,\n",
    "#                  linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "# ax_iran.add_patch(rect)\n",
    "\n",
    "ax_iran.plot([xmin, xmax, xmax, xmin, xmin],\n",
    "             [ymin, ymin, ymax, ymax, ymin],\n",
    "             color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# --- Right: Zoomed-in elevation with stations ---\n",
    "ax_zoom = fig.add_axes([0.02, 0.05, 0.8, 0.8])  # Right side\n",
    "\n",
    "iran.plot(ax=ax_zoom, facecolor='none', edgecolor='black', linewidth=1.2)\n",
    "\n",
    "elev_img = ax_zoom.imshow(zi_masked, extent=(xi.min(), xi.max(), yi.min(), yi.max()),\n",
    "                          origin='lower', cmap='terrain', alpha=0.8)\n",
    "\n",
    "# # Set zoom limits\n",
    "# ax_zoom.set_xlim(zoom_xmin, zoom_xmax)\n",
    "# ax_zoom.set_ylim(zoom_ymin, zoom_ymax)\n",
    "\n",
    "geo_df.plot(ax=ax_zoom, color='black', markersize=20, marker='^', zorder=3)\n",
    "\n",
    "for idx, row in geo_df.iterrows():\n",
    "    ax_zoom.text(row.geometry.x + 0.05, row.geometry.y, \n",
    "                 str(row['station_id']), fontsize=8, color='black')\n",
    "# ax_zoom.set_title('Zoomed Region: Elevation & Stations', fontsize=12)\n",
    "# ax_zoom.set_xticks([]); ax_zoom.set_yticks([])\n",
    "ax_zoom.set_xlim(xmin, xmax)\n",
    "ax_zoom.set_ylim(ymin, ymax)\n",
    "# ax_zoom.set_xticks([]); ax_zoom.set_yticks([])\n",
    "\n",
    "# --- Colorbar ---\n",
    "cbar = fig.colorbar(elev_img, ax=ax_zoom, orientation='vertical', pad=0.09, fraction=0.03)\n",
    "# cbar.set_label('Elevation (m)')\n",
    "\n",
    "# --- Compass Rose (top right of zoom view) ---\n",
    "compass_img = mpimg.imread('materials/vecteezy_nautical-compass-icon_.jpg')\n",
    "ax_compass = fig.add_axes([0.68, 0.77, 0.2, 0.2])  # Adjust position/size here\n",
    "ax_compass.imshow(compass_img)\n",
    "ax_compass.axis('off')  # Hide axes frame\n",
    "ax_compass.set_title(\"N\", fontsize=10, pad=-10)\n",
    "# imagebox = OffsetImage(compass_img, zoom=0.06)\n",
    "# ab = AnnotationBbox(imagebox, (0.91, 0.91), xycoords='axes fraction', frameon=False,)\n",
    "# ax_zoom.add_artist(ab)\n",
    "\n",
    "# --- Add lines from Iran map to zoomed region ---\n",
    "# Calculate rectangle corners\n",
    "corner1 = (xmin, ymin)\n",
    "corner2 = (xmax, ymax)\n",
    "\n",
    "# Line from bottom-left corner of zoom box\n",
    "con1 = ConnectionPatch(xyA=corner1, xyB=corner1, coordsA=\"data\", coordsB=\"data\",\n",
    "                       axesA=ax_iran, axesB=ax_zoom, color=\"red\", linestyle='--')\n",
    "fig.add_artist(con1)\n",
    "\n",
    "# Line from top-right corner of zoom box\n",
    "con2 = ConnectionPatch(xyA=corner2, xyB=corner2, coordsA=\"data\", coordsB=\"data\",\n",
    "                       axesA=ax_iran, axesB=ax_zoom, color=\"red\", linestyle='--')\n",
    "fig.add_artist(con2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"iran_stations.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e069888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../main_data.csv\")\n",
    "\n",
    "# Group by station and calculate averages\n",
    "summary = df.groupby(['station_id', 'station_name', 'region_name', 'station_elevation'])[\n",
    "    ['tm_m', 'tmax_m', 'tmin_m', 'rrr24']\n",
    "].mean().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary.columns = [\n",
    "    'Station ID', 'Station Name', 'Region', 'Elevation (m)',\n",
    "    'Avg Temp (¬∞C)', 'Avg Max Temp (¬∞C)', 'Avg Min Temp (¬∞C)', 'Avg Rainfall (mm/month)'\n",
    "]\n",
    "\n",
    "# Optional: Export to Excel or Word\n",
    "summary.to_excel(\"climate_summary.xlsx\", index=False)\n",
    "\n",
    "# Or, export to Word using python-docx (optional)\n",
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading('Climate Summary of Stations', 0)\n",
    "\n",
    "table = doc.add_table(rows=1, cols=len(summary.columns))\n",
    "table.style = 'Table Grid'\n",
    "\n",
    "# Add headers\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, column in enumerate(summary.columns):\n",
    "    hdr_cells[i].text = column\n",
    "\n",
    "# Add data rows\n",
    "for _, row in summary.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    for i, item in enumerate(row):\n",
    "        row_cells[i].text = str(round(item, 2)) if isinstance(item, float) else str(item)\n",
    "\n",
    "doc.save(\"climate_summary.docx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
